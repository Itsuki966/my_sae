#!/usr/bin/env python
"""
ãƒ¡ãƒ¢ãƒªç®¡ç†ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Llama3ã§ã®å‹•ä½œç¢ºèªã¨ãƒ¡ãƒ¢ãƒªä¸è¶³å¯¾ç­–ã®æ¤œè¨¼
"""

import torch
import psutil
import os
from config import ExperimentConfig, LLAMA3_MEMORY_OPTIMIZED_CONFIG, TEST_CONFIG
from sycophancy_analyzer import SycophancyAnalyzer

def check_system_memory():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’ç¢ºèª"""
    virtual_memory = psutil.virtual_memory()
    print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæƒ…å ±:")
    print(f"   - ç·ãƒ¡ãƒ¢ãƒª: {virtual_memory.total / (1024**3):.2f} GB")
    print(f"   - åˆ©ç”¨å¯èƒ½: {virtual_memory.available / (1024**3):.2f} GB")
    print(f"   - ä½¿ç”¨ç‡: {virtual_memory.percent:.1f}%")
    
    if torch.cuda.is_available():
        print("\nğŸ¯ GPUæƒ…å ±:")
        for i in range(torch.cuda.device_count()):
            total_memory = torch.cuda.get_device_properties(i).total_memory
            allocated = torch.cuda.memory_allocated(i)
            cached = torch.cuda.memory_reserved(i)
            print(f"   GPU {i}: ç·ãƒ¡ãƒ¢ãƒª {total_memory/(1024**3):.2f}GB, "
                  f"ä½¿ç”¨ä¸­ {allocated/(1024**3):.2f}GB, "
                  f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ {cached/(1024**3):.2f}GB")
    else:
        print("\nâš ï¸ CUDAä½¿ç”¨ä¸å¯ï¼ˆCPUãƒ¢ãƒ¼ãƒ‰ï¼‰")

def test_gpt2_memory_usage():
    """gpt2ã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ” GPT2ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ")
    check_system_memory()
    
    try:
        config = ExperimentConfig()
        config.model.name = "gpt2"
        config.model.sae_release = "gpt2-small-res-jb"
        config.model.sae_id = "blocks.8.hook_resid_pre"
        
        analyzer = SycophancyAnalyzer(config)
        print("\nğŸ”„ GPT2ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        
        success = analyzer.setup_models()
        if success:
            print("âœ… GPT2èª­ã¿è¾¼ã¿æˆåŠŸ")
            print(f"ğŸ”§ ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³:")
            memory_info = analyzer.get_model_memory_footprint()
            for key, value in memory_info.items():
                if isinstance(value, (int, float)):
                    print(f"   - {key}: {value:.2f}")
                else:
                    print(f"   - {key}: {value}")
        else:
            print("âŒ GPT2èª­ã¿è¾¼ã¿å¤±æ•—")
            
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del analyzer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
    except Exception as e:
        print(f"âŒ GPT2ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

def test_llama3_memory_requirements():
    """Llama3ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’æ¨å®š"""
    print("\nğŸ¦™ Llama3ãƒ¡ãƒ¢ãƒªè¦ä»¶æ¨å®š")
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
    virtual_memory = psutil.virtual_memory()
    available_gb = virtual_memory.available / (1024**3)
    
    print(f"ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_gb:.2f} GB")
    
    # Llama3-1Bã®æ¨å®šãƒ¡ãƒ¢ãƒªè¦ä»¶
    llama3_1b_estimated = 4.0  # GB (ãƒ¢ãƒ‡ãƒ« + SAE + å®Ÿè¡Œæ™‚ãƒ¡ãƒ¢ãƒª)
    llama3_3b_estimated = 12.0  # GB
    
    print(f"ğŸ“ˆ Llama3-1Bæ¨å®šè¦ä»¶: ~{llama3_1b_estimated} GB")
    print(f"ğŸ“ˆ Llama3-3Bæ¨å®šè¦ä»¶: ~{llama3_3b_estimated} GB")
    
    if available_gb < llama3_1b_estimated:
        print("âš ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆLlama3-1Bï¼‰")
        print("ğŸ’¡ æ¨å¥¨å¯¾ç­–:")
        print("   - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é…ç½®ï¼ˆCPU+GPUï¼‰ã®ä½¿ç”¨")
        print("   - float16ç²¾åº¦ã®ä½¿ç”¨")
        print("   - ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å‰Šæ¸›")
        print("   - ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã®çµ‚äº†")
    else:
        print("âœ… Llama3-1Bã¯å‹•ä½œå¯èƒ½ã¨æ€ã‚ã‚Œã¾ã™")
    
    if available_gb < llama3_3b_estimated:
        print("âš ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆLlama3-3Bï¼‰")
    else:
        print("âœ… Llama3-3Bã‚‚å‹•ä½œå¯èƒ½ã¨æ€ã‚ã‚Œã¾ã™")

def test_memory_management_functions():
    """ãƒ¡ãƒ¢ãƒªç®¡ç†é–¢æ•°ã®ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”§ ãƒ¡ãƒ¢ãƒªç®¡ç†é–¢æ•°ãƒ†ã‚¹ãƒˆ")
    
    try:
        config = ExperimentConfig()
        analyzer = SycophancyAnalyzer(config)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—ãƒ†ã‚¹ãƒˆ
        print("ğŸ“Š get_model_memory_footprint()ãƒ†ã‚¹ãƒˆ:")
        memory_info = analyzer.get_model_memory_footprint()
        for key, value in memory_info.items():
            print(f"   - {key}: {value}")
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”„ optimize_memory_usage()ãƒ†ã‚¹ãƒˆ:")
        analyzer.optimize_memory_usage()
        print("âœ… ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Œäº†")
        
        # SAEãƒ‡ãƒã‚¤ã‚¹å–å¾—ãƒ†ã‚¹ãƒˆ  
        print("\nğŸ¯ get_current_sae_device()ãƒ†ã‚¹ãƒˆ:")
        device = analyzer.get_current_sae_device()
        print(f"   ç¾åœ¨ã®SAEãƒ‡ãƒã‚¤ã‚¹: {device}")
        
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¢ãƒªç®¡ç†é–¢æ•°ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("ğŸ§ª ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    check_system_memory()
    test_gpt2_memory_usage()
    test_llama3_memory_requirements()
    test_memory_management_functions()
    
    print("\nğŸ ãƒ†ã‚¹ãƒˆå®Œäº†")
