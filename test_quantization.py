#!/usr/bin/env python3
"""
é‡å­åŒ–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

bitsandbytesã«ã‚ˆã‚‹4bit/8bité‡å­åŒ–ã®å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚
Llama3ãƒ¢ãƒ‡ãƒ«ã§ã‚µãƒ³ãƒ—ãƒ«æ•°5ã®è»½é‡ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›åŠ¹æœã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import os
import sys
import time
import traceback
import psutil
import torch
from transformers import BitsAndBytesConfig
from config import QUANTIZED_4BIT_TEST_CONFIG, QUANTIZED_8BIT_TEST_CONFIG
from sycophancy_analyzer import SycophancyAnalyzer

def check_memory_usage():
    """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    return memory_mb

def check_gpu_memory():
    """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    if torch.cuda.is_available():
        gpu_memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
        gpu_memory_reserved_mb = torch.cuda.memory_reserved() / 1024 / 1024
        return gpu_memory_mb, gpu_memory_reserved_mb
    elif torch.backends.mps.is_available():
        # MPSã§ã¯ãƒ¡ãƒ¢ãƒªæƒ…å ±ã®è©³ç´°å–å¾—ãŒåˆ¶é™ã•ã‚Œã‚‹
        return 0, 0
    else:
        return 0, 0

def create_quantization_config(config):
    """é‡å­åŒ–è¨­å®šã‚’ä½œæˆ"""
    if not config.model.use_quantization:
        return None
    
    if config.model.quantization_config == "4bit":
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=config.model.bnb_4bit_use_double_quant,
            bnb_4bit_quant_type=config.model.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=getattr(torch, config.model.bnb_4bit_compute_dtype)
        )
    elif config.model.quantization_config == "8bit":
        return BitsAndBytesConfig(
            load_in_8bit=True,
        )
    else:
        return None

def test_quantization_config(config_name, config):
    """æŒ‡å®šã•ã‚ŒãŸè¨­å®šã§é‡å­åŒ–ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª {config_name} ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"{'='*60}")
    
    # ã‚ˆã‚Šè©³ç´°ãªè¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
    print(f"ğŸ”§ ãƒ†ã‚¹ãƒˆè¨­å®šè©³ç´°:")
    print(f"   ãƒ¢ãƒ‡ãƒ«: {config.model.name}")
    print(f"   SAE: {config.model.sae_release}")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {config.data.sample_size}")
    print(f"   ãƒ‡ãƒã‚¤ã‚¹: {config.model.device}")
    if config.model.use_quantization:
        print(f"   é‡å­åŒ–: {config.model.quantization_config}")
        if config.model.load_in_4bit:
            print(f"   4bitè¨­å®š: {config.model.bnb_4bit_quant_type}, double_quant={config.model.bnb_4bit_use_double_quant}")
        if config.model.load_in_8bit:
            print(f"   8bitè¨­å®š: æœ‰åŠ¹")
    
    # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    initial_memory = check_memory_usage()
    initial_gpu_memory, initial_gpu_reserved = check_gpu_memory()
    
    print(f"\nğŸ“Š åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
    print(f"   RAM: {initial_memory:.1f} MB")
    if torch.cuda.is_available():
        print(f"   GPU: {initial_gpu_memory:.1f} MB (äºˆç´„: {initial_gpu_reserved:.1f} MB)")
    
    try:
        # é‡å­åŒ–è¨­å®šã®ä½œæˆ
        quantization_config = create_quantization_config(config)
        
        if quantization_config:
            print(f"ğŸ”§ é‡å­åŒ–è¨­å®š:")
            if config.model.load_in_4bit:
                print(f"   4bité‡å­åŒ–: æœ‰åŠ¹")
                print(f"   äºŒé‡é‡å­åŒ–: {config.model.bnb_4bit_use_double_quant}")
                print(f"   é‡å­åŒ–ã‚¿ã‚¤ãƒ—: {config.model.bnb_4bit_quant_type}")
                print(f"   è¨ˆç®—ç²¾åº¦: {config.model.bnb_4bit_compute_dtype}")
            elif config.model.load_in_8bit:
                print(f"   8bité‡å­åŒ–: æœ‰åŠ¹")
        else:
            print(f"ğŸ”§ é‡å­åŒ–: ç„¡åŠ¹")
        
        # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
        print(f"ğŸš€ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–ä¸­...")
        start_time = time.time()
        
        analyzer = SycophancyAnalyzer(config)
        init_time = time.time() - start_time
        
        # åˆæœŸåŒ–å¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        after_init_memory = check_memory_usage()
        after_init_gpu_memory, after_init_gpu_reserved = check_gpu_memory()
        
        print(f"âœ… åˆæœŸåŒ–å®Œäº† ({init_time:.1f}ç§’)")
        print(f"ğŸ“Š åˆæœŸåŒ–å¾Œãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        print(f"   RAM: {after_init_memory:.1f} MB (+{after_init_memory - initial_memory:.1f})")
        if torch.cuda.is_available():
            print(f"   GPU: {after_init_gpu_memory:.1f} MB (+{after_init_gpu_memory - initial_gpu_memory:.1f})")
            print(f"   GPUäºˆç´„: {after_init_gpu_reserved:.1f} MB (+{after_init_gpu_reserved - initial_gpu_reserved:.1f})")
        
        # ç°¡å˜ãªåˆ†æãƒ†ã‚¹ãƒˆ
        print(f"ğŸ” åˆ†æãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {config.data.dataset_path}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {config.data.sample_size}")
        start_time = time.time()
        
        try:
            results = analyzer.run_complete_analysis()
            analysis_time = time.time() - start_time
            
            # åˆ†æå¾Œã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            final_memory = check_memory_usage()
            final_gpu_memory, final_gpu_reserved = check_gpu_memory()
            
            print(f"âœ… åˆ†æå®Œäº† ({analysis_time:.1f}ç§’)")
            print(f"ğŸ“Š æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
            print(f"   RAM: {final_memory:.1f} MB")
            if torch.cuda.is_available():
                print(f"   GPU: {final_gpu_memory:.1f} MB")
                print(f"   GPUäºˆç´„: {final_gpu_reserved:.1f} MB")
            
            # çµæœã‚µãƒãƒªãƒ¼
            print(f"\nğŸ“ˆ çµæœã‚µãƒãƒªãƒ¼:")
            if 'results' in results and results['results']:
                print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(results['results'])}")
            else:
                print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: 0")
            
            if 'analysis' in results and 'sycophancy_rate' in results['analysis']:
                print(f"   è¿åˆç‡: {results['analysis']['sycophancy_rate']:.2%}")
            else:
                print(f"   è¿åˆç‡: è¨ˆç®—ä¸­ã¾ãŸã¯åˆ©ç”¨ä¸å¯")
            
            print(f"   ç·å‡¦ç†æ™‚é–“: {init_time + analysis_time:.1f}ç§’")
            
            return True, {
                'init_time': init_time,
                'analysis_time': analysis_time,
                'memory_usage': {
                    'initial': initial_memory,
                    'after_init': after_init_memory,
                    'final': final_memory
                },
                'gpu_memory_usage': {
                    'initial': initial_gpu_memory,
                    'after_init': after_init_gpu_memory,
                    'final': final_gpu_memory
                },
                'results': results
            }
            
        except Exception as analysis_error:
            analysis_time = time.time() - start_time
            print(f"âŒ åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ ({analysis_time:.1f}ç§’çµŒé): {analysis_error}")
            print(f"\nğŸ” è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±:")
            traceback.print_exc()
            
            return False, {
                'error': str(analysis_error),
                'init_time': init_time,
                'analysis_time': analysis_time
            }
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:")
        print(f"   {type(e).__name__}: {str(e)}")
        print(f"\nğŸ” è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±:")
        traceback.print_exc()
        return False, {'error': str(e)}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§ª é‡å­åŒ–æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
    print(f"PyTorch ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    
    # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
    print(f"\nğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±:")
    if torch.cuda.is_available():
        print(f"   CUDA: åˆ©ç”¨å¯èƒ½ ({torch.cuda.get_device_name()})")
        print(f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    elif torch.backends.mps.is_available():
        print(f"   MPS: åˆ©ç”¨å¯èƒ½ (Apple Silicon)")
    else:
        print(f"   CPU ã®ã¿")
    
    # bitsandbytes ã®ç¢ºèª
    try:
        import bitsandbytes as bnb
        print(f"   bitsandbytes: {bnb.__version__}")
    except ImportError:
        print("âŒ bitsandbytes ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: pip install bitsandbytes")
        return
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    test_configs = [
        ("4bité‡å­åŒ–", QUANTIZED_4BIT_TEST_CONFIG),
        ("8bité‡å­åŒ–", QUANTIZED_8BIT_TEST_CONFIG),
    ]
    
    results = {}
    
    for config_name, config in test_configs:
        success, result = test_quantization_config(config_name, config)
        results[config_name] = {
            'success': success,
            'result': result
        }
        
        # GPUãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        time.sleep(2)  # å°‘ã—å¾…æ©Ÿ
    
    # ç·åˆçµæœè¡¨ç¤º
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç·åˆãƒ†ã‚¹ãƒˆçµæœ")
    print(f"{'='*60}")
    
    for config_name, data in results.items():
        print(f"\n{config_name}:")
        if data['success']:
            result = data['result']
            print(f"  âœ… æˆåŠŸ")
            print(f"  åˆæœŸåŒ–æ™‚é–“: {result['init_time']:.1f}ç§’")
            print(f"  åˆ†ææ™‚é–“: {result['analysis_time']:.1f}ç§’")
            print(f"  æœ€å¤§RAMä½¿ç”¨é‡: {result['memory_usage']['final']:.1f} MB")
            if torch.cuda.is_available():
                print(f"  æœ€å¤§GPUä½¿ç”¨é‡: {result['gpu_memory_usage']['final']:.1f} MB")
        else:
            print(f"  âŒ å¤±æ•—: {data['result'].get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
    
    print(f"\nğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    main()
