"""
ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
CUDA 9.1ç’°å¢ƒã§flash-attnãŒä½¿ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•
"""
import torch
import os
from typing import Dict, Any, Optional

class MemoryOptimizer:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cuda_version: str = "9.1"):
        self.cuda_version = cuda_version
        self.flash_attn_available = self._check_flash_attn()
        
    def _check_flash_attn(self) -> bool:
        """flash-attnãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            import flash_attn
            return True
        except ImportError:
            return False
    
    def get_memory_config(self, model_name: str = "default") -> Dict[str, Any]:
        """ç’°å¢ƒã«å¿œã˜ãŸãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šã‚’è¿”ã™"""
        config = {
            # åŸºæœ¬è¨­å®š
            "use_fp16": True,
            "use_gradient_checkpointing": True,
            "low_cpu_mem_usage": True,
            "device_map": "auto",
            
            # CUDA 9.1ç”¨è¿½åŠ æœ€é©åŒ–
            "torch_dtype": torch.float16,
            "attn_implementation": "eager",  # flash_attnã®ä»£ã‚ã‚Š
            "max_memory_per_gpu": "6GB",  # GPUä½¿ç”¨é‡åˆ¶é™
            
            # PyTorchæœ€é©åŒ–è¨­å®š
            "torch_compile": False,  # CUDA 9.1ã§ã¯ç„¡åŠ¹
            "use_cache": True,
            "pad_token_id": None,
        }
        
        # Gemma-2Bç‰¹åŒ–ã®æœ€é©åŒ–
        if "gemma" in model_name.lower():
            config.update({
                "max_memory_per_gpu": "4GB",  # Gemma-2Bã¯ã‚ˆã‚Šå°‘ãªã„ãƒ¡ãƒ¢ãƒªã§å‹•ä½œ
                "memory_fraction": 0.7,       # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’ã‚ˆã‚Šåˆ¶é™
                "offload_to_cpu": True,       # ç©æ¥µçš„ã«CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                "use_sequential_cpu_offload": True,  # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
                "enable_cpu_offload": True,   # CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–
            })
            print("ğŸ¯ Gemma-2B specific optimizations applied")
        elif "llama" in model_name.lower():
            config.update({
                "max_memory_per_gpu": "8GB",  # Llamaã¯å°‘ã—å¤šã‚ã®ãƒ¡ãƒ¢ãƒª
                "memory_fraction": 0.8,
            })
            print("ğŸ¦™ Llama specific optimizations applied")
        
        if self.flash_attn_available:
            config["attn_implementation"] = "flash_attention_2"
        else:
            print(f"âš ï¸  flash-attn not available (CUDA {self.cuda_version}). Using eager attention.")
            
        return config
    
    def optimize_model_loading(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚ã®æœ€é©åŒ–è¨­å®š"""
        # ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—ã—ã¦ãƒ¢ãƒ‡ãƒ«ç‰¹åŒ–ã®æœ€é©åŒ–ã‚’é©ç”¨
        model_name = model_config.get("name", "default")
        memory_config = self.get_memory_config(model_name)
        
        # æ—¢å­˜è¨­å®šã«æœ€é©åŒ–è¨­å®šã‚’ãƒãƒ¼ã‚¸
        optimized_config = {**model_config, **memory_config}
        
        return optimized_config
    
    def set_memory_fraction(self, fraction: float = 0.8):
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™"""
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(fraction)
            print(f"ğŸ”§ GPU memory fraction set to {fraction}")
    
    def enable_memory_efficient_attention(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªAttentionã‚’æœ‰åŠ¹åŒ–"""
        # PyTorch 2.0ä»¥é™ã®Scaled Dot Product Attentionã‚’ä½¿ç”¨
        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
            print("âœ… Using PyTorch native scaled_dot_product_attention")
            return True
        return False
    
    def clear_cache(self):
        """GPU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU cache cleared")
    
    def optimize_for_gemma2b(self) -> Dict[str, Any]:
        """Gemma-2Bç‰¹åŒ–ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š"""
        config = self.get_memory_config("gemma-2b-it")
        
        # Gemma-2Bè¿½åŠ æœ€é©åŒ–
        gemma_specific = {
            # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«é©ã—ãŸè¨­å®š
            "max_memory": {"0": "4GB"},  # GPU 0ã«4GBåˆ¶é™
            "low_cpu_mem_usage": True,
            "use_safetensors": True,
            
            # ç”Ÿæˆæœ€é©åŒ–
            "pad_token_id": 0,  # Gemmaã®pad token
            "eos_token_id": 1,  # Gemmaã®eos token
            
            # ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
            "batch_size": 1,    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1
            "gradient_accumulation_steps": 4,  # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è“„ç©
        }
        
        config.update(gemma_specific)
        return config


def get_cuda_optimized_config() -> Dict[str, Any]:
    """CUDAç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã‚’è¿”ã™"""
    optimizer = MemoryOptimizer()
    
    base_config = {
        "use_fp16": True,
        "max_memory_gb": 6,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }
    
    return optimizer.optimize_model_loading(base_config)


def get_gemma2b_optimized_config() -> Dict[str, Any]:
    """Gemma-2Bç‰¹åŒ–ã®æœ€é©åŒ–è¨­å®šã‚’è¿”ã™"""
    optimizer = MemoryOptimizer()
    
    base_config = {
        "name": "gemma-2b-it",
        "use_fp16": True,
        "max_memory_gb": 4,  # Gemma-2Bç”¨ã«åˆ¶é™
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }
    
    return optimizer.optimize_for_gemma2b()


# ç’°å¢ƒæƒ…å ±ã®è¡¨ç¤º
def print_memory_info():
    """ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¡¨ç¤º"""
    print("ğŸ” Memory Optimization Info:")
    print(f"  - CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPU Count: {torch.cuda.device_count()}")
        print(f"  - Current GPU: {torch.cuda.get_device_name(0)}")
        print(f"  - CUDA Version: {torch.version.cuda}")
        
        # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
        cached_memory = torch.cuda.memory_reserved(0) / 1024**3
        
        print(f"  - Total GPU Memory: {total_memory:.2f} GB")
        print(f"  - Allocated Memory: {allocated_memory:.2f} GB")
        print(f"  - Cached Memory: {cached_memory:.2f} GB")
    
    optimizer = MemoryOptimizer()
    print(f"  - Flash Attention: {'âœ… Available' if optimizer.flash_attn_available else 'âŒ Not Available'}")


if __name__ == "__main__":
    print_memory_info()
    config = get_cuda_optimized_config()
    print(f"\nğŸš€ Optimized Config: {config}")