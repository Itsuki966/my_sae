"""
ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
CUDA 9.1ç’°å¢ƒã§flash-attnãŒä½¿ç”¨ã§ããªã„å ´åˆã®ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•
"""
import torch
import os
from typing import Dict, Any, Optional

class MemoryOptimizer:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cuda_version: str = "9.1"):
        self.cuda_version = cuda_version
        self.flash_attn_available = self._check_flash_attn()
        
    def _check_flash_attn(self) -> bool:
        """flash-attnãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            import flash_attn
            return True
        except ImportError:
            return False
    
    def get_memory_config(self) -> Dict[str, Any]:
        """ç’°å¢ƒã«å¿œã˜ãŸãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šã‚’è¿”ã™"""
        config = {
            # åŸºæœ¬è¨­å®š
            "use_fp16": True,
            "use_gradient_checkpointing": True,
            "low_cpu_mem_usage": True,
            "device_map": "auto",
            
            # CUDA 9.1ç”¨è¿½åŠ æœ€é©åŒ–
            "torch_dtype": torch.float16,
            "attn_implementation": "eager",  # flash_attnã®ä»£ã‚ã‚Š
            "max_memory_per_gpu": "6GB",  # GPUä½¿ç”¨é‡åˆ¶é™
            
            # PyTorchæœ€é©åŒ–è¨­å®š
            "torch_compile": False,  # CUDA 9.1ã§ã¯ç„¡åŠ¹
            "use_cache": True,
            "pad_token_id": None,
        }
        
        if self.flash_attn_available:
            config["attn_implementation"] = "flash_attention_2"
        else:
            print(f"âš ï¸  flash-attn not available (CUDA {self.cuda_version}). Using eager attention.")
            
        return config
    
    def optimize_model_loading(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚ã®æœ€é©åŒ–è¨­å®š"""
        memory_config = self.get_memory_config()
        
        # æ—¢å­˜è¨­å®šã«æœ€é©åŒ–è¨­å®šã‚’ãƒãƒ¼ã‚¸
        optimized_config = {**model_config, **memory_config}
        
        return optimized_config
    
    def set_memory_fraction(self, fraction: float = 0.8):
        """GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶é™"""
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(fraction)
            print(f"ğŸ”§ GPU memory fraction set to {fraction}")
    
    def enable_memory_efficient_attention(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªAttentionã‚’æœ‰åŠ¹åŒ–"""
        # PyTorch 2.0ä»¥é™ã®Scaled Dot Product Attentionã‚’ä½¿ç”¨
        if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
            print("âœ… Using PyTorch native scaled_dot_product_attention")
            return True
        return False
    
    def clear_cache(self):
        """GPU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU cache cleared")


def get_cuda_optimized_config() -> Dict[str, Any]:
    """CUDAç’°å¢ƒã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šã‚’è¿”ã™"""
    optimizer = MemoryOptimizer()
    
    base_config = {
        "use_fp16": True,
        "max_memory_gb": 6,
        "device": "cuda" if torch.cuda.is_available() else "cpu"
    }
    
    return optimizer.optimize_model_loading(base_config)


# ç’°å¢ƒæƒ…å ±ã®è¡¨ç¤º
def print_memory_info():
    """ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’è¡¨ç¤º"""
    print("ğŸ” Memory Optimization Info:")
    print(f"  - CUDA Available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - GPU Count: {torch.cuda.device_count()}")
        print(f"  - Current GPU: {torch.cuda.get_device_name(0)}")
        print(f"  - CUDA Version: {torch.version.cuda}")
        
        # GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
        cached_memory = torch.cuda.memory_reserved(0) / 1024**3
        
        print(f"  - Total GPU Memory: {total_memory:.2f} GB")
        print(f"  - Allocated Memory: {allocated_memory:.2f} GB")
        print(f"  - Cached Memory: {cached_memory:.2f} GB")
    
    optimizer = MemoryOptimizer()
    print(f"  - Flash Attention: {'âœ… Available' if optimizer.flash_attn_available else 'âŒ Not Available'}")


if __name__ == "__main__":
    print_memory_info()
    config = get_cuda_optimized_config()
    print(f"\nğŸš€ Optimized Config: {config}")