#!/usr/bin/env python3
"""
LLMç”Ÿæˆå•é¡Œã®ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€LLMã®ç”ŸæˆãŒç©ºã«ãªã‚‹åŸå› ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã®
æ®µéšçš„ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import torch
import os
import sys
from typing import Optional

# ç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

try:
    from sae_lens import SAE, HookedSAETransformer
    SAE_AVAILABLE = True
except ImportError:
    print("âŒ SAE LensãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    SAE_AVAILABLE = False
    sys.exit(1)

from config import TEST_CONFIG, LLAMA3_TEST_CONFIG

class LLMGenerationDebugger:
    """LLMç”Ÿæˆå•é¡Œã®ãƒ‡ãƒãƒƒã‚°ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.config = config
        self.model = None
        self.tokenizer = None
        self.device = config.model.device
        
    def test_basic_model_loading(self):
        """åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
        print("=" * 50)
        print("ğŸ” åŸºæœ¬çš„ãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        try:
            print(f"ğŸ“± ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
            print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {self.config.model.name}")
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            print("ğŸ”„ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
            self.model = HookedSAETransformer.from_pretrained(
                self.config.model.name,
                device=self.device
            )
            print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            
            # Tokenizerã®å–å¾—
            self.tokenizer = self.model.tokenizer
            print("âœ… Tokenizerå–å¾—æˆåŠŸ")
            
            # åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
            print(f"ğŸ”§ èªå½™ã‚µã‚¤ã‚º: {self.tokenizer.vocab_size}")
            print(f"ğŸ”§ EOSãƒˆãƒ¼ã‚¯ãƒ³ID: {self.tokenizer.eos_token_id}")
            print(f"ğŸ”§ ãƒ‘ãƒƒãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ID: {getattr(self.tokenizer, 'pad_token_id', 'None')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_simple_tokenization(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ†ã‚¹ãƒˆ"""
        print("\n" + "=" * 50)
        print("ğŸ” ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        test_text = "Hello, world!"
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆ: '{test_text}'")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            tokens = self.tokenizer.encode(test_text, return_tensors="pt")
            print(f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³: {tokens}")
            print(f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {tokens.shape[1]}")
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            decoded = self.tokenizer.decode(tokens[0], skip_special_tokens=True)
            print(f"ğŸ“ ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: '{decoded}'")
            
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•
            tokens_on_device = tokens.to(self.device)
            print(f"ğŸ“± ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•: {tokens.device} â†’ {tokens_on_device.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_simple_generation(self):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        print("\n" + "=" * 50)
        print("ğŸ” ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        test_prompt = "The capital of France is"
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: '{test_prompt}'")
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(test_prompt, return_tensors="pt").to(self.device)
            original_length = inputs.shape[1]
            print(f"ğŸ”¢ å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {original_length}")
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆï¼ˆæœ€å°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
            with torch.no_grad():
                try:
                    outputs = self.model.generate(
                        inputs,
                        max_new_tokens=5,  # æœ€å°é™ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
                        temperature=1.0,   # æ¨™æº–çš„ãªæ¸©åº¦
                        do_sample=False,   # ã‚°ãƒªãƒ¼ãƒ‡ã‚£ç”Ÿæˆ
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                    
                    # ç”Ÿæˆã•ã‚ŒãŸéƒ¨åˆ†ã‚’å–å¾—
                    generated_part = outputs[0][original_length:]
                    response = self.tokenizer.decode(generated_part, skip_special_tokens=True)
                    
                    print(f"âœ… ç”ŸæˆæˆåŠŸ: '{response}'")
                    return True
                    
                except Exception as gen_error:
                    print(f"âš ï¸ generate()ãƒ¡ã‚½ãƒƒãƒ‰ã‚¨ãƒ©ãƒ¼: {gen_error}")
                    return self.test_manual_generation(inputs, original_length)
                    
        except Exception as e:
            print(f"âŒ ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_manual_generation(self, inputs: torch.Tensor, original_length: int):
        """æ‰‹å‹•ç”Ÿæˆãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        print("ğŸ”„ æ‰‹å‹•ç”Ÿæˆãƒ†ã‚¹ãƒˆã«åˆ‡ã‚Šæ›¿ãˆ")
        
        try:
            generated_tokens = inputs.clone()
            
            with torch.no_grad():
                for step in range(3):  # 3ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ç”Ÿæˆ
                    # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
                    logits = self.model(generated_tokens)
                    next_token_logits = logits[0, -1, :]
                    
                    # ã‚°ãƒªãƒ¼ãƒ‡ã‚£é¸æŠ
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
                    generated_tokens = torch.cat([generated_tokens, next_token.unsqueeze(0)], dim=1)
                    
                    # ä¸­é–“çµæœè¡¨ç¤º
                    current_text = self.tokenizer.decode(
                        generated_tokens[0, original_length:], 
                        skip_special_tokens=True
                    )
                    print(f"ã‚¹ãƒ†ãƒƒãƒ— {step + 1}: '{current_text}'")
                    
                    # EOSãƒã‚§ãƒƒã‚¯
                    if next_token.item() == self.tokenizer.eos_token_id:
                        print("ğŸ›‘ EOSãƒˆãƒ¼ã‚¯ãƒ³ã§çµ‚äº†")
                        break
            
            # æœ€çµ‚çµæœ
            final_text = self.tokenizer.decode(
                generated_tokens[0, original_length:], 
                skip_special_tokens=True
            )
            print(f"âœ… æ‰‹å‹•ç”ŸæˆæˆåŠŸ: '{final_text}'")
            return True
            
        except Exception as e:
            print(f"âŒ æ‰‹å‹•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def test_choice_question_generation(self):
        """é¸æŠè‚¢è³ªå•ã§ã®ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        print("\n" + "=" * 50)
        print("ğŸ” é¸æŠè‚¢è³ªå•ã§ã®ç”Ÿæˆãƒ†ã‚¹ãƒˆ")
        print("=" * 50)
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªé¸æŠè‚¢è³ªå•
        prompt = """Question: What is 2 + 2?

Options:
(A) 3
(B) 4
(C) 5

Select the best answer. Respond with only the letter (A-C).
"""
        
        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
        print(prompt)
        print("-" * 30)
        
        try:
            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            original_length = inputs.shape[1]
            
            # è¤‡æ•°ã®ç”Ÿæˆè¨­å®šã§ãƒ†ã‚¹ãƒˆ
            test_configs = [
                {"max_new_tokens": 1, "temperature": 0.0, "do_sample": False},
                {"max_new_tokens": 3, "temperature": 0.0, "do_sample": False},
                {"max_new_tokens": 5, "temperature": 0.1, "do_sample": True},
                {"max_new_tokens": 10, "temperature": 0.5, "do_sample": True},
            ]
            
            for i, config in enumerate(test_configs):
                print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆè¨­å®š {i+1}: {config}")
                
                try:
                    with torch.no_grad():
                        outputs = self.model.generate(
                            inputs,
                            **config,
                            pad_token_id=self.tokenizer.eos_token_id
                        )
                        
                        generated_part = outputs[0][original_length:]
                        response = self.tokenizer.decode(generated_part, skip_special_tokens=True)
                        
                        print(f"  ç”Ÿæˆçµæœ: '{response}'")
                        print(f"  é•·ã•: {len(response)} æ–‡å­—")
                        
                        if response.strip():
                            print("  âœ… éç©ºã®å¿œç­”")
                        else:
                            print("  âŒ ç©ºã®å¿œç­”")
                            
                except Exception as e:
                    print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            
            return True
            
        except Exception as e:
            print(f"âŒ é¸æŠè‚¢è³ªå•ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        print("ğŸš€ LLMç”Ÿæˆãƒ‡ãƒãƒƒã‚°ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹")
        
        tests = [
            ("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿", self.test_basic_model_loading),
            ("ãƒˆãƒ¼ã‚¯ãƒ³åŒ–", self.test_simple_tokenization),
            ("ã‚·ãƒ³ãƒ—ãƒ«ç”Ÿæˆ", self.test_simple_generation),
            ("é¸æŠè‚¢è³ªå•ç”Ÿæˆ", self.test_choice_question_generation),
        ]
        
        results = {}
        
        for test_name, test_func in tests:
            if test_name in ["ãƒˆãƒ¼ã‚¯ãƒ³åŒ–", "ã‚·ãƒ³ãƒ—ãƒ«ç”Ÿæˆ", "é¸æŠè‚¢è³ªå•ç”Ÿæˆ"] and not hasattr(self, 'model'):
                print(f"â­ï¸ {test_name}ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—ã®ãŸã‚ï¼‰")
                results[test_name] = False
                continue
                
            try:
                success = test_func()
                results[test_name] = success
            except Exception as e:
                print(f"âŒ {test_name}ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                results[test_name] = False
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 50)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        for test_name, success in results.items():
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±æ•—"
            print(f"{test_name}: {status}")
        
        # æ¨å¥¨äº‹é …
        print("\nğŸ”§ æ¨å¥¨ã•ã‚Œã‚‹ä¿®æ­£:")
        if not results.get("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿", False):
            print("- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å‡¦ç†ã®ä¿®æ­£ãŒå¿…è¦")
        if not results.get("ã‚·ãƒ³ãƒ—ãƒ«ç”Ÿæˆ", False):
            print("- ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå¿…è¦")
            print("- ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã®ç¢ºèªãŒå¿…è¦")
        if results.get("ã‚·ãƒ³ãƒ—ãƒ«ç”Ÿæˆ", False) and not results.get("é¸æŠè‚¢è³ªå•ç”Ÿæˆ", False):
            print("- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã®æœ€é©åŒ–ãŒå¿…è¦")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ”¬ LLMç”Ÿæˆå•é¡Œãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    
    # GPT-2ã®ãƒ†ã‚¹ãƒˆ
    print("\nğŸ¤– GPT-2ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ")
    gpt2_debugger = LLMGenerationDebugger(TEST_CONFIG)
    gpt2_debugger.run_all_tests()
    
    # Llama3ã®ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    print("\nğŸ¦™ Llama3ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ")
    try:
        llama_debugger = LLMGenerationDebugger(LLAMA3_TEST_CONFIG)
        llama_debugger.run_all_tests()
    except Exception as e:
        print(f"âš ï¸ Llama3ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")

if __name__ == "__main__":
    main()
