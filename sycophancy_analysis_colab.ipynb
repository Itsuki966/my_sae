{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a14bf3",
   "metadata": {},
   "source": [
    "## ⚠️ 重要な更新情報\n",
    "\n",
    "**🚀 新しい改善版ランチャーが利用可能です！**\n",
    "\n",
    "Google Colab環境での**依存関係インストールのハングアップ問題**を根本的に解決するため、新しい`launcher_colab.ipynb`を作成しました。\n",
    "\n",
    "### 📋 主要な改善点\n",
    "- ✅ **段階的依存関係インストール**: `pip install -r requirements.txt`のハングアップを解決\n",
    "- ✅ **環境構築と実験実行の分離**: 問題の切り分けが容易\n",
    "- ✅ **Git sparse-checkout**: 必要ファイルのみ高速取得\n",
    "- ✅ **包括的デバッグ支援**: エラー時の迅速な解決\n",
    "\n",
    "### 🎯 推奨使用方法\n",
    "1. **新規実験**: `launcher_colab.ipynb`を使用してください\n",
    "2. **既存実験**: このノートブックも引き続き使用可能ですが、問題発生時は新版を推奨\n",
    "\n",
    "### 📁 ファイル構成\n",
    "```\n",
    "launcher_colab.ipynb          # 🆕 推奨: メインランチャー\n",
    "requirements-colab.txt        # 🆕 Colab最適化版依存関係\n",
    "sycophancy_analysis_colab.ipynb  # このファイル（引き続き使用可能）\n",
    "README_COLAB_IMPROVEMENTS.md # 🆕 詳細な改善ガイド\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8254d",
   "metadata": {},
   "source": [
    "# 🧠 LLM迎合性分析 - Google Colab版\n",
    "\n",
    "このノートブックは、Google Colab環境で**Git sparse checkout**を使用してLLMの迎合性（Sycophancy）分析を実行します。\n",
    "\n",
    "## 🎯 特徴\n",
    "- **軽量設計**: メインコードをGitリポジトリから直接インポート\n",
    "- **自動更新**: 最新のコード改善が自動反映\n",
    "- **Gemma-2B対応**: 効率的な2Bパラメータモデルをサポート\n",
    "- **T4 GPU最適化**: Google Colab T4環境での最適パフォーマンス\n",
    "\n",
    "## 📋 実行順序\n",
    "1. **環境セットアップ**: Git clone & 依存関係インストール\n",
    "2. **モデル選択**: 使用するLLMモデルを選択\n",
    "3. **分析実行**: 迎合性分析の実行\n",
    "4. **結果可視化**: 分析結果の詳細表示\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ 重要**: このノートブックはGoogle Colab環境での使用を前提としています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda6e50",
   "metadata": {},
   "source": [
    "## 🎯 使用方法とカスタマイズ\n",
    "\n",
    "### ✅ **基本的な使用手順**\n",
    "\n",
    "1. **環境セットアップセル**を実行\n",
    "   - Git sparse checkoutでコードを取得\n",
    "   - 必要な依存関係をインストール\n",
    "\n",
    "2. **メインコードインポートセル**でモデルを選択\n",
    "   ```python\n",
    "   # 使用したいモデル設定を選択\n",
    "   SELECTED_CONFIG = GEMMA2B_TEST_CONFIG     # Gemma-2B\n",
    "   # SELECTED_CONFIG = LLAMA3_TEST_CONFIG    # Llama-3.2-1B  \n",
    "   ```\n",
    "\n",
    "3. **分析実行セル**で迎合性分析を実行\n",
    "   - 全て自動で実行（モデル読み込み〜分析〜結果保存）\n",
    "\n",
    "4. **可視化セル**で結果を確認\n",
    "   - インタラクティブなグラフ表示\n",
    "   - 詳細統計と具体例の表示\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 **高度なカスタマイズ**\n",
    "\n",
    "#### **モデル変更**\n",
    "```python\n",
    "# Gemma-2B（推奨 - T4 GPUに最適）\n",
    "SELECTED_CONFIG = GEMMA2B_TEST_CONFIG          # テスト用（少数サンプル）\n",
    "SELECTED_CONFIG = GEMMA2B_MEMORY_OPTIMIZED_CONFIG  # メモリ効率重視\n",
    "\n",
    "# Llama-3.2-1B\n",
    "SELECTED_CONFIG = LLAMA3_TEST_CONFIG           # テスト用\n",
    "SELECTED_CONFIG = LLAMA3_MEMORY_OPTIMIZED_CONFIG   # メモリ効率重視\n",
    "\n",
    "# 自動選択（環境に応じて最適な設定）\n",
    "SELECTED_CONFIG = get_auto_config()\n",
    "```\n",
    "\n",
    "#### **サンプルサイズ調整**\n",
    "```python\n",
    "# config.pyから設定をコピーして調整\n",
    "from copy import deepcopy\n",
    "custom_config = deepcopy(GEMMA2B_TEST_CONFIG)\n",
    "custom_config.data.sample_size = 100  # サンプル数を変更\n",
    "SELECTED_CONFIG = custom_config\n",
    "```\n",
    "\n",
    "#### **詳細デバッグ**\n",
    "```python\n",
    "# デバッグモードを有効化\n",
    "custom_config.debug.verbose = True\n",
    "custom_config.debug.show_prompts = True\n",
    "custom_config.debug.show_responses = True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **対応モデル**\n",
    "\n",
    "| モデル | 推奨設定 | メモリ使用量 | 特徴 |\n",
    "|--------|----------|--------------|------|\n",
    "| **Gemma-2B** | `GEMMA2B_TEST_CONFIG` | ~4-6GB | T4最適、高効率 |\n",
    "| **Llama-3.2-1B** | `LLAMA3_TEST_CONFIG` | ~3-5GB | 軽量、高性能 |  \n",
    "| **Llama-3.2-3B** | `LLAMA3_MEMORY_OPTIMIZED_CONFIG` | ~8-12GB | 高性能、要量子化 |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ **トラブルシューティング**\n",
    "\n",
    "**Git cloneエラー:**\n",
    "- ネットワーク接続を確認\n",
    "- リポジトリのアクセス権限を確認\n",
    "\n",
    "**メモリ不足エラー:**\n",
    "- より小さなモデル（Gemma-2B）を使用\n",
    "- メモリ最適化設定を選択\n",
    "- サンプルサイズを削減\n",
    "\n",
    "**インポートエラー:**\n",
    "- 環境セットアップセルを再実行\n",
    "- ランタイムを再起動\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 **更新とメンテナンス**\n",
    "\n",
    "このノートブックは**Git sparse checkout**を使用しているため：\n",
    "\n",
    "✅ **自動更新**: メインリポジトリの改善が自動反映  \n",
    "✅ **軽量**: 必要ファイルのみダウンロード  \n",
    "✅ **最新**: 常に最新のバグ修正と機能改善を利用  \n",
    "\n",
    "メインコードに更新がある場合は、環境セットアップセルを再実行するだけで最新版が取得されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe03b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 環境セットアップ & Git Sparse Checkout\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"Google Colab環境のセットアップとGit sparse checkoutの実行\"\"\"\n",
    "    \n",
    "    # Google Colab環境の確認\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"✅ Google Colab環境を検出\")\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        print(\"⚠️  ローカル環境での実行を検出\")\n",
    "        IN_COLAB = False\n",
    "    \n",
    "    # GPU情報の表示\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"🚀 GPU検出: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "    else:\n",
    "        print(\"❌ GPU利用不可 - CPUモードで実行\")\n",
    "    \n",
    "    return IN_COLAB\n",
    "\n",
    "def git_sparse_checkout(branch=\"main\"):\n",
    "    \"\"\"Git sparse checkoutでリポジトリから必要ファイルのみを取得（改良版）\"\"\"\n",
    "    \n",
    "    print(f\"📂 Git sparse checkoutを開始... (ブランチ: {branch})\")\n",
    "    \n",
    "    # 現在のディレクトリを保存（Google Colab対応）\n",
    "    original_dir = os.getcwd()\n",
    "    print(f\"🔍 元のディレクトリ: {original_dir}\")\n",
    "    \n",
    "    # 既存のクローンディレクトリがあれば削除（正しいディレクトリ名を使用）\n",
    "    if os.path.exists(\"my_sae\"):\n",
    "        !rm -rf my_sae\n",
    "        print(\"🧹 既存のクローンディレクトリを削除\")\n",
    "    \n",
    "    try:\n",
    "        # リポジトリのクローン（filter=blob:noneで高速化、checkoutなし）\n",
    "        print(f\"📥 リポジトリをクローン中... (ブランチ: {branch})\")\n",
    "        !git clone --filter=blob:none --no-checkout -b {branch} https://github.com/Itsuki966/my_sae.git\n",
    "        \n",
    "        # ディレクトリ移動\n",
    "        os.chdir(\"my_sae\")\n",
    "        print(f\"🔍 作業ディレクトリ変更: {os.getcwd()}\")\n",
    "\n",
    "        # Sparse checkout の初期化（coneモードで高速化）\n",
    "        print(\"🔧 Sparse checkout を設定中...\")\n",
    "        !git sparse-checkout init --cone\n",
    "        \n",
    "        # 必要なファイル・ディレクトリを指定（改良版requirements-colab.txtを含む）\n",
    "        files_to_checkout = [\n",
    "            \"sycophancy_analyzer.py\",\n",
    "            \"config.py\", \n",
    "            \"memory_optimizer.py\",\n",
    "            \"eval_dataset\",\n",
    "            \"requirements-colab.txt\",  # 最適化版を優先使用\n",
    "            \"requirements.txt\",        # フォールバック用\n",
    "            \"pyproject.toml\"           # 参考情報用\n",
    "        ]\n",
    "        \n",
    "        checkout_list = \" \".join(files_to_checkout)\n",
    "        !git sparse-checkout set {checkout_list}\n",
    "        \n",
    "        # 実際のチェックアウト実行\n",
    "        print(\"📋 必要ファイルをチェックアウト中...\")\n",
    "        !git checkout\n",
    "        \n",
    "        # 現在のコミットハッシュを表示（実験再現性のため）\n",
    "        result = !git rev-parse --short HEAD\n",
    "        commit_hash = result[0] if result else \"unknown\"\n",
    "        print(f\"📌 使用コミット: {commit_hash}\")\n",
    "        \n",
    "        # 元のディレクトリに戻る（重要：Google Colab対応）\n",
    "        os.chdir(original_dir)\n",
    "        print(f\"🔍 ディレクトリ復元: {os.getcwd()}\")\n",
    "        \n",
    "        # チェックアウトされたファイルの確認\n",
    "        print(f\"\\n✅ 取得完了ファイル:\")\n",
    "        for item in files_to_checkout:\n",
    "            item_path = os.path.join(\"my_sae\", item)\n",
    "            if os.path.exists(item_path):\n",
    "                if os.path.isfile(item_path):\n",
    "                    size = os.path.getsize(item_path) / 1024  # KB\n",
    "                    print(f\"   📄 {item} ({size:.1f}KB)\")\n",
    "                else:\n",
    "                    try:\n",
    "                        file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
    "                        print(f\"   📁 {item}/ ({file_count}ファイル)\")\n",
    "                    except:\n",
    "                        print(f\"   📁 {item}/ (ディレクトリ)\")\n",
    "            else:\n",
    "                print(f\"   ❌ {item} (見つかりません)\")\n",
    "        \n",
    "        return True, commit_hash\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Git sparse checkout エラー: {e}\")\n",
    "        print(f\"💡 ブランチ'{branch}'が存在するか確認してください\")\n",
    "        print(f\"💡 ネットワーク接続を確認してください\")\n",
    "        # エラー時も元のディレクトリに戻る\n",
    "        try:\n",
    "            os.chdir(original_dir)\n",
    "        except:\n",
    "            pass\n",
    "        return False, None\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"必要な依存関係の段階的インストール（ハングアップ問題対策版）\"\"\"\n",
    "    \n",
    "    print(\"📦 依存関係をインストール中...\")\n",
    "    \n",
    "    # Colab最適化版requirements.txtを優先使用\n",
    "    colab_requirements = \"my_sae/requirements-colab.txt\"\n",
    "    fallback_requirements = \"my_sae/requirements.txt\"\n",
    "    \n",
    "    # 段階的インストール戦略\n",
    "    if os.path.exists(colab_requirements):\n",
    "        print(f\"🎯 Colab最適化版を使用: {colab_requirements}\")\n",
    "        strategy = \"colab_optimized\"\n",
    "    elif os.path.exists(fallback_requirements):\n",
    "        print(f\"📋 標準版を使用: {fallback_requirements}\")\n",
    "        strategy = \"standard\"\n",
    "    else:\n",
    "        print(\"⚠️ requirements.txtが見つかりません - 手動インストールに切り替え\")\n",
    "        strategy = \"manual\"\n",
    "    \n",
    "    success = False\n",
    "    \n",
    "    if strategy == \"colab_optimized\":\n",
    "        success = install_colab_optimized(colab_requirements)\n",
    "    elif strategy == \"standard\":\n",
    "        success = install_with_timeout_protection(fallback_requirements)\n",
    "    else:  # manual\n",
    "        success = install_core_manually()\n",
    "    \n",
    "    if success:\n",
    "        print(\"✅ 依存関係インストール完了\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ 依存関係インストールに失敗\")\n",
    "        print(\"💡 手動インストール方法:\")\n",
    "        print(\"   1. 上記のエラーメッセージを確認\")\n",
    "        print(\"   2. 個別にライブラリをインストール: !pip install <package_name>\")\n",
    "        print(\"   3. 必要に応じてランタイムを再起動\")\n",
    "        return False\n",
    "\n",
    "def install_colab_optimized(requirements_file):\n",
    "    \"\"\"Colab最適化版requirements.txtを使用したインストール\"\"\"\n",
    "    print(\"🚀 Colab最適化版依存関係をインストール中...\")\n",
    "    \n",
    "    try:\n",
    "        # タイムアウト保護付きでインストール\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 最大20分のタイムアウト\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-r', requirements_file],\n",
    "            timeout=1200,  # 20分\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Colab最適化版インストール完了 ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Colab最適化版インストール失敗\")\n",
    "            if result.stderr:\n",
    "                print(f\"エラー: {result.stderr[:300]}...\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⏰ Colab最適化版インストール - タイムアウト (20分)\")\n",
    "        print(\"💡 手動インストールに切り替えます...\")\n",
    "        return install_core_manually()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Colab最適化版インストール例外: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_with_timeout_protection(requirements_file):\n",
    "    \"\"\"タイムアウト保護付きの標準インストール\"\"\"\n",
    "    print(\"⚠️ 標準版requirements.txtを使用（タイムアウト保護付き）...\")\n",
    "    \n",
    "    try:\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 最大30分のタイムアウト\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-r', requirements_file],\n",
    "            timeout=1800,  # 30分\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ 標準版インストール完了 ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ 標準版インストール失敗\")\n",
    "            if result.stderr:\n",
    "                print(f\"エラー: {result.stderr[:300]}...\")\n",
    "            print(\"💡 手動インストールに切り替えます...\")\n",
    "            return install_core_manually()\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⏰ 標準版インストール - タイムアウト (30分)\")\n",
    "        print(\"💡 手動インストールに切り替えます...\")\n",
    "        return install_core_manually()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 標準版インストール例外: {e}\")\n",
    "        return install_core_manually()\n",
    "\n",
    "def install_core_manually():\n",
    "    \"\"\"核心ライブラリの手動段階的インストール\"\"\"\n",
    "    print(\"🔧 核心ライブラリを手動でインストール中...\")\n",
    "    \n",
    "    # 必須ライブラリのリスト（優先順位順）\n",
    "    core_libraries = [\n",
    "        (\"torch\", \"PyTorch (GPU対応)\"),\n",
    "        (\"transformers\", \"HuggingFace Transformers\"),\n",
    "        (\"sae-lens\", \"SAE Lens (核心ライブラリ)\"),\n",
    "        (\"accelerate\", \"メモリ効率化\"),\n",
    "        (\"plotly\", \"可視化\"),\n",
    "        (\"tqdm\", \"プログレスバー\")\n",
    "    ]\n",
    "    \n",
    "    failed_libraries = []\n",
    "    \n",
    "    for library, description in core_libraries:\n",
    "        print(f\"📦 {description} をインストール中...\")\n",
    "        try:\n",
    "            !pip install -q {library}\n",
    "            print(f\"   ✅ {library} 完了\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ {library} 失敗: {e}\")\n",
    "            failed_libraries.append(library)\n",
    "    \n",
    "    if failed_libraries:\n",
    "        print(f\"⚠️ 失敗したライブラリ: {', '.join(failed_libraries)}\")\n",
    "        print(\"💡 これらのライブラリは後で個別に再試行してください\")\n",
    "        return len(failed_libraries) < len(core_libraries) // 2  # 半数以上成功すれば継続\n",
    "    else:\n",
    "        print(\"✅ 全ての核心ライブラリのインストール完了\")\n",
    "        return True\n",
    "\n",
    "# 🔄 実験設定（ブランチ選択）\n",
    "# ===============================================\n",
    "EXPERIMENT_BRANCH = \"main\"  # ここを変更して異なるブランチを使用\n",
    "# EXPERIMENT_BRANCH = \"feature/new-model-support\"    # 新機能ブランチ\n",
    "# EXPERIMENT_BRANCH = \"experiment/temperature-test\"  # 実験ブランチ\n",
    "# EXPERIMENT_BRANCH = \"develop\"                       # 開発ブランチ\n",
    "\n",
    "# 環境セットアップの実行\n",
    "print(\"🚀 Google Colab環境セットアップを開始...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "IN_COLAB = setup_colab_environment()\n",
    "\n",
    "# Git sparse checkoutの実行\n",
    "success, commit_hash = git_sparse_checkout(EXPERIMENT_BRANCH)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\n📦 依存関係をインストール中...\")\n",
    "    install_dependencies()\n",
    "    \n",
    "    # パスの追加（インポート用）- Google Colab環境対応\n",
    "    repo_path = os.path.abspath(\"my_sae\")\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.insert(0, repo_path)\n",
    "        print(f\"📁 Pythonパスに追加: {repo_path}\")\n",
    "    \n",
    "    # Colab環境では /content/ もパスに追加\n",
    "    if IN_COLAB:\n",
    "        content_path = \"/content\"\n",
    "        content_repo_path = \"/content/my_sae\"\n",
    "        if content_path not in sys.path:\n",
    "            sys.path.insert(0, content_path)\n",
    "        if content_repo_path not in sys.path:\n",
    "            sys.path.insert(0, content_repo_path)\n",
    "        print(f\"📁 Colab用パス追加: {content_path}, {content_repo_path}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ 環境セットアップ完了！\")\n",
    "    print(f\"🔖 使用ブランチ: {EXPERIMENT_BRANCH}\")\n",
    "    print(f\"📌 コミットハッシュ: {commit_hash}\")\n",
    "    print(\"💡 次のセルでメインコードをインポートしてください\")\n",
    "    \n",
    "    # 実験記録用のグローバル変数\n",
    "    EXPERIMENT_METADATA = {\n",
    "        'branch': EXPERIMENT_BRANCH,\n",
    "        'commit_hash': commit_hash,\n",
    "        'setup_time': __import__('datetime').datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ セットアップに失敗しました\")\n",
    "    print(f\"💡 ネットワーク接続またはブランチ'{EXPERIMENT_BRANCH}'の存在を確認してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 メインコードのインポートと設定\n",
    "try:\n",
    "    # メインの分析クラスをインポート\n",
    "    from sycophancy_analyzer import SycophancyAnalyzer\n",
    "    \n",
    "    # 設定をインポート\n",
    "    from config import (\n",
    "        GEMMA2B_TEST_CONFIG, GEMMA2B_PROD_CONFIG, GEMMA2B_MEMORY_OPTIMIZED_CONFIG,\n",
    "        LLAMA3_TEST_CONFIG, LLAMA3_MEMORY_OPTIMIZED_CONFIG,\n",
    "        TEST_CONFIG, DEFAULT_CONFIG,\n",
    "        get_auto_config\n",
    "    )\n",
    "    \n",
    "    print(\"✅ メインコードのインポート完了\")\n",
    "    print(\"📋 利用可能な設定:\")\n",
    "    print(\"   🔸 GEMMA2B_TEST_CONFIG - Gemma-2B テスト用（少数サンプル）\")\n",
    "    print(\"   🔸 GEMMA2B_PROD_CONFIG - Gemma-2B 本番用（大規模分析）\") \n",
    "    print(\"   🔸 GEMMA2B_MEMORY_OPTIMIZED_CONFIG - Gemma-2B メモリ最適化版\")\n",
    "    print(\"   🔸 LLAMA3_TEST_CONFIG - Llama-3.2-1B テスト用\")\n",
    "    print(\"   🔸 LLAMA3_MEMORY_OPTIMIZED_CONFIG - Llama-3.2-1B メモリ最適化版\")\n",
    "    print(\"   🔸 TEST_CONFIG - GPT-2 テスト用\")\n",
    "    print(\"   🔸 get_auto_config() - 環境に応じた自動選択\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"❌ インポートエラー: {e}\")\n",
    "    print(\"💡 上記の環境セットアップセルを先に実行してください\")\n",
    "    raise\n",
    "\n",
    "# 🎯 高速実験設定（ここを変更して即座に実験条件を変更）\n",
    "# ===============================================\n",
    "\n",
    "# 🔧 実験設定のカスタマイズ機能\n",
    "def create_custom_config(base_config, **kwargs):\n",
    "    \"\"\"設定を動的にカスタマイズする関数\"\"\"\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    custom_config = deepcopy(base_config)\n",
    "    \n",
    "    # 動的に設定を変更\n",
    "    if 'sample_size' in kwargs:\n",
    "        custom_config.data.sample_size = kwargs['sample_size']\n",
    "    if 'model_name' in kwargs:\n",
    "        custom_config.model.name = kwargs['model_name']\n",
    "    if 'max_new_tokens' in kwargs:\n",
    "        custom_config.generation.max_new_tokens = kwargs['max_new_tokens']\n",
    "    if 'temperature' in kwargs:\n",
    "        custom_config.generation.temperature = kwargs['temperature']\n",
    "    if 'verbose' in kwargs:\n",
    "        custom_config.debug.verbose = kwargs['verbose']\n",
    "    \n",
    "    return custom_config\n",
    "\n",
    "# 🚀 実験設定（この部分を変更して即座に実験条件を変更）\n",
    "# ===============================================\n",
    "\n",
    "# 基本設定の選択\n",
    "BASE_CONFIG = GEMMA2B_TEST_CONFIG  # ベース設定\n",
    "\n",
    "# 実験パラメータの上書き（ここを自由に変更）\n",
    "EXPERIMENT_PARAMS = {\n",
    "    'sample_size': 10,        # サンプル数（1-1000）\n",
    "    'temperature': 0.3,       # 生成温度（0.0-1.0）\n",
    "    'max_new_tokens': 3,      # 最大生成トークン数\n",
    "    'verbose': True,          # 詳細ログ出力\n",
    "}\n",
    "\n",
    "# カスタム設定の作成\n",
    "SELECTED_CONFIG = create_custom_config(BASE_CONFIG, **EXPERIMENT_PARAMS)\n",
    "\n",
    "print(f\"\\n🎯 実験設定:\")\n",
    "print(f\"   📱 ベースモデル: {BASE_CONFIG.model.name}\")\n",
    "print(f\"   \udcca サンプル数: {SELECTED_CONFIG.data.sample_size}\")\n",
    "print(f\"   🌡️  温度: {SELECTED_CONFIG.generation.temperature}\")\n",
    "print(f\"   \udd22 最大トークン: {SELECTED_CONFIG.generation.max_new_tokens}\")\n",
    "print(f\"   🔍 詳細ログ: {SELECTED_CONFIG.debug.verbose}\")\n",
    "print(f\"   🔍 SAE: {SELECTED_CONFIG.model.sae_release}\")\n",
    "print(f\"   🖥️  デバイス: {SELECTED_CONFIG.model.device}\")\n",
    "\n",
    "# 🔄 ワンクリック実験変更例\n",
    "print(f\"\\n🔄 高速実験変更例:\")\n",
    "print(f\"# 大規模実験に変更\")\n",
    "print(f\"EXPERIMENT_PARAMS['sample_size'] = 100\")\n",
    "print(f\"\")\n",
    "print(f\"# 高温度生成実験\")  \n",
    "print(f\"EXPERIMENT_PARAMS['temperature'] = 0.8\")\n",
    "print(f\"\")\n",
    "print(f\"# より長い生成実験\")\n",
    "print(f\"EXPERIMENT_PARAMS['max_new_tokens'] = 10\")\n",
    "\n",
    "# メモリ最適化設定\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(f\"\\n✅ 設定完了 - 次のセルで分析を実行してください\")\n",
    "print(f\"💡 EXPERIMENT_PARAMSを変更してこのセルを再実行すると即座に設定が変更されます\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 LLM迎合性分析の実行\n",
    "print(\"🚀 LLM迎合性分析を開始...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 実験メタデータの準備\n",
    "experiment_metadata = {\n",
    "    'experiment_start': __import__('datetime').datetime.now().isoformat(),\n",
    "    'branch': EXPERIMENT_METADATA.get('branch', 'unknown') if 'EXPERIMENT_METADATA' in globals() else 'unknown',\n",
    "    'commit_hash': EXPERIMENT_METADATA.get('commit_hash', 'unknown') if 'EXPERIMENT_METADATA' in globals() else 'unknown',\n",
    "    'config_params': {\n",
    "        'model_name': SELECTED_CONFIG.model.name,\n",
    "        'sample_size': SELECTED_CONFIG.data.sample_size,\n",
    "        'temperature': SELECTED_CONFIG.generation.temperature,\n",
    "        'max_new_tokens': SELECTED_CONFIG.generation.max_new_tokens,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"📋 実験メタデータ:\")\n",
    "print(f\"   🌿 ブランチ: {experiment_metadata['branch']}\")\n",
    "print(f\"   📌 コミット: {experiment_metadata['commit_hash']}\")\n",
    "print(f\"   ⏰ 開始時刻: {experiment_metadata['experiment_start']}\")\n",
    "print(f\"   📊 設定: {experiment_metadata['config_params']}\")\n",
    "\n",
    "try:\n",
    "    # 分析器の初期化\n",
    "    print(f\"\\n🔧 分析器を初期化中...\")\n",
    "    analyzer = SycophancyAnalyzer(SELECTED_CONFIG)\n",
    "    \n",
    "    # メモリ使用状況の表示\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"💾 初期GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "        experiment_metadata['initial_memory_gb'] = round(memory_used, 2)\n",
    "    \n",
    "    # 完全分析の実行（モデル読み込み + 分析 + 結果保存を一括実行）\n",
    "    print(f\"\\n🔄 完全分析を実行中...\")\n",
    "    print(f\"   📋 この処理には数分かかる場合があります...\")\n",
    "    \n",
    "    # 実行時間測定開始\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # メイン分析の実行\n",
    "    analyzer.run_complete_analysis()\n",
    "    \n",
    "    # 実行時間測定終了\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    experiment_metadata['execution_time_seconds'] = round(execution_time, 2)\n",
    "    experiment_metadata['experiment_end'] = __import__('datetime').datetime.now().isoformat()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"✅ 分析完了！\")\n",
    "    print(f\"⏱️  実行時間: {execution_time:.1f}秒\")\n",
    "    \n",
    "    # 結果の簡易表示\n",
    "    if hasattr(analyzer, 'analysis_results') and analyzer.analysis_results:\n",
    "        stats = analyzer.analysis_results\n",
    "        \n",
    "        # 実験メタデータに結果を追加\n",
    "        experiment_metadata['results'] = {\n",
    "            'total_samples': stats.get('total_samples', 0),\n",
    "            'sycophancy_rate': stats.get('sycophancy_rate', 0),\n",
    "            'initial_accuracy': stats.get('initial_accuracy', 0),\n",
    "            'challenge_accuracy': stats.get('challenge_accuracy', 0),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📊 分析結果サマリー:\")\n",
    "        print(f\"   🎯 使用モデル: {stats.get('model_name', 'N/A')}\")\n",
    "        print(f\"   📝 総サンプル数: {stats.get('total_samples', 0)}\")\n",
    "        print(f\"   📈 迎合率: {stats.get('sycophancy_rate', 0):.1%}\")\n",
    "        print(f\"   📈 初回正答率: {stats.get('initial_accuracy', 0):.1%}\")\n",
    "        print(f\"   📈 チャレンジ後正答率: {stats.get('challenge_accuracy', 0):.1%}\")\n",
    "    \n",
    "    # メモリ使用状況の最終確認\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        print(f\"\\n💾 最終GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "        experiment_metadata['final_memory_gb'] = round(memory_used, 2)\n",
    "    \n",
    "    # 実験ログの保存\n",
    "    import json\n",
    "    experiment_log_file = f\"experiment_log_{experiment_metadata['experiment_start'][:19].replace(':', '-')}.json\"\n",
    "    with open(experiment_log_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n📋 実験ログ保存: {experiment_log_file}\")\n",
    "    print(f\"💡 詳細な結果は次のセルで可視化されます\")\n",
    "    \n",
    "    # グローバル変数に結果を保存（可視化用）\n",
    "    analysis_results = analyzer.analysis_results if hasattr(analyzer, 'analysis_results') else {}\n",
    "    detailed_results = analyzer.results if hasattr(analyzer, 'results') else []\n",
    "    \n",
    "    # 実験メタデータもグローバルに保存\n",
    "    current_experiment_metadata = experiment_metadata\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 分析実行エラー: {e}\")\n",
    "    print(f\"\\n💡 トラブルシューティング:\")\n",
    "    print(f\"   1. GPU メモリが不足していないか確認\")\n",
    "    print(f\"   2. より小さなサンプルサイズの設定を試行\")\n",
    "    print(f\"   3. ランタイムを再起動して再実行\")\n",
    "    print(f\"   4. 異なるブランチを試行（EXPERIMENT_BRANCH変更）\")\n",
    "    \n",
    "    # エラー情報を実験メタデータに記録\n",
    "    experiment_metadata['error'] = str(e)\n",
    "    experiment_metadata['status'] = 'failed'\n",
    "    \n",
    "    # エラー時のメモリクリア\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    raise\n",
    "\n",
    "print(f\"\\n🎉 分析セクション完了 - 次のセルで結果を可視化してください\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3027af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 分析結果の詳細可視化\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 結果データの確認\n",
    "try:\n",
    "    if 'analysis_results' in globals() and 'detailed_results' in globals():\n",
    "        stats = analysis_results\n",
    "        results = detailed_results\n",
    "        \n",
    "        print(\"📈 分析結果の可視化を開始...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not stats or not results:\n",
    "            print(\"❌ 可視化用データが不足しています\")\n",
    "            print(\"💡 上記の分析実行セルを先に実行してください\")\n",
    "        else:\n",
    "            # 1. 基本統計の表示\n",
    "            print(f\"📊 詳細統計:\")\n",
    "            print(f\"   🎯 使用モデル: {stats.get('model_name', 'N/A')}\")\n",
    "            print(f\"   📝 総サンプル数: {stats.get('total_samples', 0)}\")\n",
    "            print(f\"   🔄 迎合事例数: {stats.get('sycophantic_responses', 0)}\")\n",
    "            print(f\"   📈 迎合率: {stats.get('sycophancy_rate', 0):.1%}\")\n",
    "            print(f\"   📈 初回正答率: {stats.get('initial_accuracy', 0):.1%}\")\n",
    "            print(f\"   📈 チャレンジ後正答率: {stats.get('challenge_accuracy', 0):.1%}\")\n",
    "            \n",
    "            # 2. インタラクティブな可視化（Plotly）\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=['迎合率分析', '正答率比較', '回答変化パターン', '迎合時の正答率'],\n",
    "                specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "                       [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "            )\n",
    "            \n",
    "            # サブプロット1: 迎合率（パイチャート）\n",
    "            sycophantic_count = stats.get('sycophantic_responses', 0)\n",
    "            consistent_count = stats.get('total_samples', 0) - sycophantic_count\n",
    "            \n",
    "            fig.add_trace(go.Pie(\n",
    "                labels=['一貫した回答', '迎合的回答'],\n",
    "                values=[consistent_count, sycophantic_count],\n",
    "                hole=0.3\n",
    "            ), row=1, col=1)\n",
    "            \n",
    "            # サブプロット2: 正答率比較\n",
    "            accuracies = [stats.get('initial_accuracy', 0), stats.get('challenge_accuracy', 0)]\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=['初回回答', 'チャレンジ後'],\n",
    "                y=accuracies,\n",
    "                text=[f'{acc:.1%}' for acc in accuracies],\n",
    "                textposition='auto'\n",
    "            ), row=1, col=2)\n",
    "            \n",
    "            # サブプロット3: 回答変化パターン\n",
    "            if results:\n",
    "                pattern_counts = {'正解→正解': 0, '正解→不正解': 0, '不正解→正解': 0, '不正解→不正解': 0}\n",
    "                for result in results:\n",
    "                    initial_correct = result.get('is_initial_correct', False)\n",
    "                    challenge_correct = result.get('is_challenge_correct', False)\n",
    "                    \n",
    "                    if initial_correct and challenge_correct:\n",
    "                        pattern_counts['正解→正解'] += 1\n",
    "                    elif initial_correct and not challenge_correct:\n",
    "                        pattern_counts['正解→不正解'] += 1\n",
    "                    elif not initial_correct and challenge_correct:\n",
    "                        pattern_counts['不正解→正解'] += 1\n",
    "                    else:\n",
    "                        pattern_counts['不正解→不正解'] += 1\n",
    "                \n",
    "                fig.add_trace(go.Bar(\n",
    "                    x=list(pattern_counts.keys()),\n",
    "                    y=list(pattern_counts.values()),\n",
    "                    text=list(pattern_counts.values()),\n",
    "                    textposition='auto'\n",
    "                ), row=2, col=1)\n",
    "            \n",
    "            # サブプロット4: 迎合時の正答率詳細\n",
    "            if results:\n",
    "                sycophantic_cases = [r for r in results if r.get('is_sycophantic', False)]\n",
    "                if sycophantic_cases:\n",
    "                    syc_initial_correct = sum(1 for r in sycophantic_cases if r.get('is_initial_correct', False))\n",
    "                    syc_challenge_correct = sum(1 for r in sycophantic_cases if r.get('is_challenge_correct', False))\n",
    "                    syc_total = len(sycophantic_cases)\n",
    "                    \n",
    "                    syc_accuracies = [syc_initial_correct/syc_total, syc_challenge_correct/syc_total] if syc_total > 0 else [0, 0]\n",
    "                    \n",
    "                    fig.add_trace(go.Bar(\n",
    "                        x=['迎合時初回', '迎合時チャレンジ'],\n",
    "                        y=syc_accuracies,\n",
    "                        text=[f'{acc:.1%}' for acc in syc_accuracies],\n",
    "                        textposition='auto',\n",
    "                        marker_color='orange'\n",
    "                    ), row=2, col=2)\n",
    "            \n",
    "            # レイアウト調整\n",
    "            fig.update_layout(\n",
    "                title_text=f\"LLM迎合性分析結果 - {stats.get('model_name', 'Unknown Model')}\",\n",
    "                height=800,\n",
    "                showlegend=False\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # 3. 具体例の表示（最初の3件）\n",
    "            if results:\n",
    "                print(f\"\\n📝 分析事例（最初の3件）:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results[:3]):\n",
    "                    print(f\"\\n事例 {i+1}:\")\n",
    "                    print(f\"  質問: {result.get('question', 'N/A')[:60]}...\")\n",
    "                    print(f\"  正答: {result.get('correct_answer', 'N/A')}\")\n",
    "                    print(f\"  初回回答: {result.get('initial_choice', 'N/A')} ({'正解' if result.get('is_initial_correct') else '不正解'})\")\n",
    "                    print(f\"  チャレンジ回答: {result.get('challenge_choice', 'N/A')} ({'正解' if result.get('is_challenge_correct') else '不正解'})\")\n",
    "                    print(f\"  迎合判定: {'あり' if result.get('is_sycophantic') else 'なし'}\")\n",
    "            \n",
    "            # 4. 結果ファイルの保存情報\n",
    "            print(f\"\\n💾 結果保存:\")\n",
    "            print(f\"   📁 結果ファイルは 'results/' ディレクトリに保存されました\")\n",
    "            print(f\"   📊 可視化図表は 'plots/' ディレクトリに保存されました\")\n",
    "            \n",
    "            print(f\"\\n\" + \"=\" * 50)\n",
    "            print(\"✅ 可視化完了！\")\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ 分析結果が見つかりません\")\n",
    "        print(\"💡 上記の分析実行セルを先に実行してください\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 可視化エラー: {e}\")\n",
    "    print(\"💡 分析が正常に完了しているか確認してください\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
