{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a14bf3",
   "metadata": {},
   "source": [
    "## âš ï¸ é‡è¦ãªæ›´æ–°æƒ…å ±\n",
    "\n",
    "**ğŸš€ æ–°ã—ã„æ”¹å–„ç‰ˆãƒ©ãƒ³ãƒãƒ£ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã§ã™ï¼**\n",
    "\n",
    "Google Colabç’°å¢ƒã§ã®**ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—å•é¡Œ**ã‚’æ ¹æœ¬çš„ã«è§£æ±ºã™ã‚‹ãŸã‚ã€æ–°ã—ã„`launcher_colab.ipynb`ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n",
    "\n",
    "### ğŸ“‹ ä¸»è¦ãªæ”¹å–„ç‚¹\n",
    "- âœ… **æ®µéšçš„ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: `pip install -r requirements.txt`ã®ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—ã‚’è§£æ±º\n",
    "- âœ… **ç’°å¢ƒæ§‹ç¯‰ã¨å®Ÿé¨“å®Ÿè¡Œã®åˆ†é›¢**: å•é¡Œã®åˆ‡ã‚Šåˆ†ã‘ãŒå®¹æ˜“\n",
    "- âœ… **Git sparse-checkout**: å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿é«˜é€Ÿå–å¾—\n",
    "- âœ… **åŒ…æ‹¬çš„ãƒ‡ãƒãƒƒã‚°æ”¯æ´**: ã‚¨ãƒ©ãƒ¼æ™‚ã®è¿…é€Ÿãªè§£æ±º\n",
    "\n",
    "### ğŸ¯ æ¨å¥¨ä½¿ç”¨æ–¹æ³•\n",
    "1. **æ–°è¦å®Ÿé¨“**: `launcher_colab.ipynb`ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\n",
    "2. **æ—¢å­˜å®Ÿé¨“**: ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚‚å¼•ãç¶šãä½¿ç”¨å¯èƒ½ã§ã™ãŒã€å•é¡Œç™ºç”Ÿæ™‚ã¯æ–°ç‰ˆã‚’æ¨å¥¨\n",
    "\n",
    "### ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ\n",
    "```\n",
    "launcher_colab.ipynb          # ğŸ†• æ¨å¥¨: ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒãƒ£ãƒ¼\n",
    "requirements-colab.txt        # ğŸ†• Colabæœ€é©åŒ–ç‰ˆä¾å­˜é–¢ä¿‚\n",
    "sycophancy_analysis_colab.ipynb  # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¼•ãç¶šãä½¿ç”¨å¯èƒ½ï¼‰\n",
    "README_COLAB_IMPROVEMENTS.md # ğŸ†• è©³ç´°ãªæ”¹å–„ã‚¬ã‚¤ãƒ‰\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca8254d",
   "metadata": {},
   "source": [
    "# ğŸ§  LLMè¿åˆæ€§åˆ†æ - Google Colabç‰ˆ\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Google Colabç’°å¢ƒã§**Git sparse checkout**ã‚’ä½¿ç”¨ã—ã¦LLMã®è¿åˆæ€§ï¼ˆSycophancyï¼‰åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ç‰¹å¾´\n",
    "- **è»½é‡è¨­è¨ˆ**: ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’Gitãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "- **è‡ªå‹•æ›´æ–°**: æœ€æ–°ã®ã‚³ãƒ¼ãƒ‰æ”¹å–„ãŒè‡ªå‹•åæ˜ \n",
    "- **Gemma-2Bå¯¾å¿œ**: åŠ¹ç‡çš„ãª2Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆ\n",
    "- **T4 GPUæœ€é©åŒ–**: Google Colab T4ç’°å¢ƒã§ã®æœ€é©ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹\n",
    "\n",
    "## ğŸ“‹ å®Ÿè¡Œé †åº\n",
    "1. **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: Git clone & ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "2. **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ\n",
    "3. **åˆ†æå®Ÿè¡Œ**: è¿åˆæ€§åˆ†æã®å®Ÿè¡Œ\n",
    "4. **çµæœå¯è¦–åŒ–**: åˆ†æçµæœã®è©³ç´°è¡¨ç¤º\n",
    "\n",
    "---\n",
    "\n",
    "**âš ï¸ é‡è¦**: ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯Google Colabç’°å¢ƒã§ã®ä½¿ç”¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda6e50",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä½¿ç”¨æ–¹æ³•ã¨ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "\n",
    "### âœ… **åŸºæœ¬çš„ãªä½¿ç”¨æ‰‹é †**\n",
    "\n",
    "1. **ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«**ã‚’å®Ÿè¡Œ\n",
    "   - Git sparse checkoutã§ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—\n",
    "   - å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "2. **ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚»ãƒ«**ã§ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ\n",
    "   ```python\n",
    "   # ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’é¸æŠ\n",
    "   SELECTED_CONFIG = GEMMA2B_TEST_CONFIG     # Gemma-2B\n",
    "   # SELECTED_CONFIG = LLAMA3_TEST_CONFIG    # Llama-3.2-1B  \n",
    "   ```\n",
    "\n",
    "3. **åˆ†æå®Ÿè¡Œã‚»ãƒ«**ã§è¿åˆæ€§åˆ†æã‚’å®Ÿè¡Œ\n",
    "   - å…¨ã¦è‡ªå‹•ã§å®Ÿè¡Œï¼ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã€œåˆ†æã€œçµæœä¿å­˜ï¼‰\n",
    "\n",
    "4. **å¯è¦–åŒ–ã‚»ãƒ«**ã§çµæœã‚’ç¢ºèª\n",
    "   - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚°ãƒ©ãƒ•è¡¨ç¤º\n",
    "   - è©³ç´°çµ±è¨ˆã¨å…·ä½“ä¾‹ã®è¡¨ç¤º\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º**\n",
    "\n",
    "#### **ãƒ¢ãƒ‡ãƒ«å¤‰æ›´**\n",
    "```python\n",
    "# Gemma-2Bï¼ˆæ¨å¥¨ - T4 GPUã«æœ€é©ï¼‰\n",
    "SELECTED_CONFIG = GEMMA2B_TEST_CONFIG          # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆå°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ï¼‰\n",
    "SELECTED_CONFIG = GEMMA2B_MEMORY_OPTIMIZED_CONFIG  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–\n",
    "\n",
    "# Llama-3.2-1B\n",
    "SELECTED_CONFIG = LLAMA3_TEST_CONFIG           # ãƒ†ã‚¹ãƒˆç”¨\n",
    "SELECTED_CONFIG = LLAMA3_MEMORY_OPTIMIZED_CONFIG   # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–\n",
    "\n",
    "# è‡ªå‹•é¸æŠï¼ˆç’°å¢ƒã«å¿œã˜ã¦æœ€é©ãªè¨­å®šï¼‰\n",
    "SELECTED_CONFIG = get_auto_config()\n",
    "```\n",
    "\n",
    "#### **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºèª¿æ•´**\n",
    "```python\n",
    "# config.pyã‹ã‚‰è¨­å®šã‚’ã‚³ãƒ”ãƒ¼ã—ã¦èª¿æ•´\n",
    "from copy import deepcopy\n",
    "custom_config = deepcopy(GEMMA2B_TEST_CONFIG)\n",
    "custom_config.data.sample_size = 100  # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¤‰æ›´\n",
    "SELECTED_CONFIG = custom_config\n",
    "```\n",
    "\n",
    "#### **è©³ç´°ãƒ‡ãƒãƒƒã‚°**\n",
    "```python\n",
    "# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–\n",
    "custom_config.debug.verbose = True\n",
    "custom_config.debug.show_prompts = True\n",
    "custom_config.debug.show_responses = True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š **å¯¾å¿œãƒ¢ãƒ‡ãƒ«**\n",
    "\n",
    "| ãƒ¢ãƒ‡ãƒ« | æ¨å¥¨è¨­å®š | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | ç‰¹å¾´ |\n",
    "|--------|----------|--------------|------|\n",
    "| **Gemma-2B** | `GEMMA2B_TEST_CONFIG` | ~4-6GB | T4æœ€é©ã€é«˜åŠ¹ç‡ |\n",
    "| **Llama-3.2-1B** | `LLAMA3_TEST_CONFIG` | ~3-5GB | è»½é‡ã€é«˜æ€§èƒ½ |  \n",
    "| **Llama-3.2-3B** | `LLAMA3_MEMORY_OPTIMIZED_CONFIG` | ~8-12GB | é«˜æ€§èƒ½ã€è¦é‡å­åŒ– |\n",
    "\n",
    "---\n",
    "\n",
    "### âš ï¸ **ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**\n",
    "\n",
    "**Git cloneã‚¨ãƒ©ãƒ¼:**\n",
    "- ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèª\n",
    "- ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã‚’ç¢ºèª\n",
    "\n",
    "**ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼:**\n",
    "- ã‚ˆã‚Šå°ã•ãªãƒ¢ãƒ‡ãƒ«ï¼ˆGemma-2Bï¼‰ã‚’ä½¿ç”¨\n",
    "- ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®šã‚’é¸æŠ\n",
    "- ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›\n",
    "\n",
    "**ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼:**\n",
    "- ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ\n",
    "- ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ **æ›´æ–°ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹**\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**Git sparse checkout**ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ï¼š\n",
    "\n",
    "âœ… **è‡ªå‹•æ›´æ–°**: ãƒ¡ã‚¤ãƒ³ãƒªãƒã‚¸ãƒˆãƒªã®æ”¹å–„ãŒè‡ªå‹•åæ˜   \n",
    "âœ… **è»½é‡**: å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰  \n",
    "âœ… **æœ€æ–°**: å¸¸ã«æœ€æ–°ã®ãƒã‚°ä¿®æ­£ã¨æ©Ÿèƒ½æ”¹å–„ã‚’åˆ©ç”¨  \n",
    "\n",
    "ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã«æ›´æ–°ãŒã‚ã‚‹å ´åˆã¯ã€ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã™ã‚‹ã ã‘ã§æœ€æ–°ç‰ˆãŒå–å¾—ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe03b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & Git Sparse Checkout\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "def setup_colab_environment():\n",
    "    \"\"\"Google Colabç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨Git sparse checkoutã®å®Ÿè¡Œ\"\"\"\n",
    "    \n",
    "    # Google Colabç’°å¢ƒã®ç¢ºèª\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"âœ… Google Colabç’°å¢ƒã‚’æ¤œå‡º\")\n",
    "        IN_COLAB = True\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Ÿè¡Œã‚’æ¤œå‡º\")\n",
    "        IN_COLAB = False\n",
    "    \n",
    "    # GPUæƒ…å ±ã®è¡¨ç¤º\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"ğŸš€ GPUæ¤œå‡º: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "    else:\n",
    "        print(\"âŒ GPUåˆ©ç”¨ä¸å¯ - CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ\")\n",
    "    \n",
    "    return IN_COLAB\n",
    "\n",
    "def git_sparse_checkout(branch=\"main\"):\n",
    "    \"\"\"Git sparse checkoutã§ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“‚ Git sparse checkoutã‚’é–‹å§‹... (ãƒ–ãƒ©ãƒ³ãƒ: {branch})\")\n",
    "    \n",
    "    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿å­˜ï¼ˆGoogle Colabå¯¾å¿œï¼‰\n",
    "    original_dir = os.getcwd()\n",
    "    print(f\"ğŸ” å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {original_dir}\")\n",
    "    \n",
    "    # æ—¢å­˜ã®ã‚¯ãƒ­ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã‚ã‚Œã°å‰Šé™¤ï¼ˆæ­£ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½¿ç”¨ï¼‰\n",
    "    if os.path.exists(\"my_sae\"):\n",
    "        !rm -rf my_sae\n",
    "        print(\"ğŸ§¹ æ—¢å­˜ã®ã‚¯ãƒ­ãƒ¼ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤\")\n",
    "    \n",
    "    try:\n",
    "        # ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆfilter=blob:noneã§é«˜é€ŸåŒ–ã€checkoutãªã—ï¼‰\n",
    "        print(f\"ğŸ“¥ ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­... (ãƒ–ãƒ©ãƒ³ãƒ: {branch})\")\n",
    "        !git clone --filter=blob:none --no-checkout -b {branch} https://github.com/Itsuki966/my_sae.git\n",
    "        \n",
    "        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç§»å‹•\n",
    "        os.chdir(\"my_sae\")\n",
    "        print(f\"ğŸ” ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤‰æ›´: {os.getcwd()}\")\n",
    "\n",
    "        # Sparse checkout ã®åˆæœŸåŒ–ï¼ˆconeãƒ¢ãƒ¼ãƒ‰ã§é«˜é€ŸåŒ–ï¼‰\n",
    "        print(\"ğŸ”§ Sparse checkout ã‚’è¨­å®šä¸­...\")\n",
    "        !git sparse-checkout init --cone\n",
    "        \n",
    "        # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼ˆæ”¹è‰¯ç‰ˆrequirements-colab.txtã‚’å«ã‚€ï¼‰\n",
    "        files_to_checkout = [\n",
    "            \"sycophancy_analyzer.py\",\n",
    "            \"config.py\", \n",
    "            \"memory_optimizer.py\",\n",
    "            \"eval_dataset\",\n",
    "            \"requirements-colab.txt\",  # æœ€é©åŒ–ç‰ˆã‚’å„ªå…ˆä½¿ç”¨\n",
    "            \"requirements.txt\",        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨\n",
    "            \"pyproject.toml\"           # å‚è€ƒæƒ…å ±ç”¨\n",
    "        ]\n",
    "        \n",
    "        checkout_list = \" \".join(files_to_checkout)\n",
    "        !git sparse-checkout set {checkout_list}\n",
    "        \n",
    "        # å®Ÿéš›ã®ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆå®Ÿè¡Œ\n",
    "        print(\"ğŸ“‹ å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆä¸­...\")\n",
    "        !git checkout\n",
    "        \n",
    "        # ç¾åœ¨ã®ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã‚’è¡¨ç¤ºï¼ˆå®Ÿé¨“å†ç¾æ€§ã®ãŸã‚ï¼‰\n",
    "        result = !git rev-parse --short HEAD\n",
    "        commit_hash = result[0] if result else \"unknown\"\n",
    "        print(f\"ğŸ“Œ ä½¿ç”¨ã‚³ãƒŸãƒƒãƒˆ: {commit_hash}\")\n",
    "        \n",
    "        # å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã‚‹ï¼ˆé‡è¦ï¼šGoogle Colabå¯¾å¿œï¼‰\n",
    "        os.chdir(original_dir)\n",
    "        print(f\"ğŸ” ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¾©å…ƒ: {os.getcwd()}\")\n",
    "        \n",
    "        # ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "        print(f\"\\nâœ… å–å¾—å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "        for item in files_to_checkout:\n",
    "            item_path = os.path.join(\"my_sae\", item)\n",
    "            if os.path.exists(item_path):\n",
    "                if os.path.isfile(item_path):\n",
    "                    size = os.path.getsize(item_path) / 1024  # KB\n",
    "                    print(f\"   ğŸ“„ {item} ({size:.1f}KB)\")\n",
    "                else:\n",
    "                    try:\n",
    "                        file_count = len([f for f in os.listdir(item_path) if os.path.isfile(os.path.join(item_path, f))])\n",
    "                        print(f\"   ğŸ“ {item}/ ({file_count}ãƒ•ã‚¡ã‚¤ãƒ«)\")\n",
    "                    except:\n",
    "                        print(f\"   ğŸ“ {item}/ (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)\")\n",
    "            else:\n",
    "                print(f\"   âŒ {item} (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n",
    "        \n",
    "        return True, commit_hash\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Git sparse checkout ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(f\"ğŸ’¡ ãƒ–ãƒ©ãƒ³ãƒ'{branch}'ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "        print(f\"ğŸ’¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "        # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã‚‹\n",
    "        try:\n",
    "            os.chdir(original_dir)\n",
    "        except:\n",
    "            pass\n",
    "        return False, None\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—å•é¡Œå¯¾ç­–ç‰ˆï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    \n",
    "    # Colabæœ€é©åŒ–ç‰ˆrequirements.txtã‚’å„ªå…ˆä½¿ç”¨\n",
    "    colab_requirements = \"my_sae/requirements-colab.txt\"\n",
    "    fallback_requirements = \"my_sae/requirements.txt\"\n",
    "    \n",
    "    # æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆ¦ç•¥\n",
    "    if os.path.exists(colab_requirements):\n",
    "        print(f\"ğŸ¯ Colabæœ€é©åŒ–ç‰ˆã‚’ä½¿ç”¨: {colab_requirements}\")\n",
    "        strategy = \"colab_optimized\"\n",
    "    elif os.path.exists(fallback_requirements):\n",
    "        print(f\"ğŸ“‹ æ¨™æº–ç‰ˆã‚’ä½¿ç”¨: {fallback_requirements}\")\n",
    "        strategy = \"standard\"\n",
    "    else:\n",
    "        print(\"âš ï¸ requirements.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆ\")\n",
    "        strategy = \"manual\"\n",
    "    \n",
    "    success = False\n",
    "    \n",
    "    if strategy == \"colab_optimized\":\n",
    "        success = install_colab_optimized(colab_requirements)\n",
    "    elif strategy == \"standard\":\n",
    "        success = install_with_timeout_protection(fallback_requirements)\n",
    "    else:  # manual\n",
    "        success = install_core_manually()\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âŒ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—\")\n",
    "        print(\"ğŸ’¡ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•:\")\n",
    "        print(\"   1. ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèª\")\n",
    "        print(\"   2. å€‹åˆ¥ã«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: !pip install <package_name>\")\n",
    "        print(\"   3. å¿…è¦ã«å¿œã˜ã¦ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•\")\n",
    "        return False\n",
    "\n",
    "def install_colab_optimized(requirements_file):\n",
    "    \"\"\"Colabæœ€é©åŒ–ç‰ˆrequirements.txtã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    print(\"ğŸš€ Colabæœ€é©åŒ–ç‰ˆä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    \n",
    "    try:\n",
    "        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¿è­·ä»˜ãã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # æœ€å¤§20åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-r', requirements_file],\n",
    "            timeout=1200,  # 20åˆ†\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Colabæœ€é©åŒ–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº† ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Colabæœ€é©åŒ–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—\")\n",
    "            if result.stderr:\n",
    "                print(f\"ã‚¨ãƒ©ãƒ¼: {result.stderr[:300]}...\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° Colabæœ€é©åŒ–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (20åˆ†)\")\n",
    "        print(\"ğŸ’¡ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...\")\n",
    "        return install_core_manually()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Colabæœ€é©åŒ–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¾‹å¤–: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_with_timeout_protection(requirements_file):\n",
    "    \"\"\"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¿è­·ä»˜ãã®æ¨™æº–ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    print(\"âš ï¸ æ¨™æº–ç‰ˆrequirements.txtã‚’ä½¿ç”¨ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¿è­·ä»˜ãï¼‰...\")\n",
    "    \n",
    "    try:\n",
    "        import subprocess\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # æœ€å¤§30åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-r', requirements_file],\n",
    "            timeout=1800,  # 30åˆ†\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… æ¨™æº–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº† ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ æ¨™æº–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—\")\n",
    "            if result.stderr:\n",
    "                print(f\"ã‚¨ãƒ©ãƒ¼: {result.stderr[:300]}...\")\n",
    "            print(\"ğŸ’¡ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...\")\n",
    "            return install_core_manually()\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° æ¨™æº–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (30åˆ†)\")\n",
    "        print(\"ğŸ’¡ æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...\")\n",
    "        return install_core_manually()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨™æº–ç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¾‹å¤–: {e}\")\n",
    "        return install_core_manually()\n",
    "\n",
    "def install_core_manually():\n",
    "    \"\"\"æ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æ‰‹å‹•æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    print(\"ğŸ”§ æ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    \n",
    "    # å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒªã‚¹ãƒˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰\n",
    "    core_libraries = [\n",
    "        (\"torch\", \"PyTorch (GPUå¯¾å¿œ)\"),\n",
    "        (\"transformers\", \"HuggingFace Transformers\"),\n",
    "        (\"sae-lens\", \"SAE Lens (æ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒª)\"),\n",
    "        (\"accelerate\", \"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–\"),\n",
    "        (\"plotly\", \"å¯è¦–åŒ–\"),\n",
    "        (\"tqdm\", \"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\")\n",
    "    ]\n",
    "    \n",
    "    failed_libraries = []\n",
    "    \n",
    "    for library, description in core_libraries:\n",
    "        print(f\"ğŸ“¦ {description} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "        try:\n",
    "            !pip install -q {library}\n",
    "            print(f\"   âœ… {library} å®Œäº†\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ {library} å¤±æ•—: {e}\")\n",
    "            failed_libraries.append(library)\n",
    "    \n",
    "    if failed_libraries:\n",
    "        print(f\"âš ï¸ å¤±æ•—ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒª: {', '.join(failed_libraries)}\")\n",
    "        print(\"ğŸ’¡ ã“ã‚Œã‚‰ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯å¾Œã§å€‹åˆ¥ã«å†è©¦è¡Œã—ã¦ãã ã•ã„\")\n",
    "        return len(failed_libraries) < len(core_libraries) // 2  # åŠæ•°ä»¥ä¸ŠæˆåŠŸã™ã‚Œã°ç¶™ç¶š\n",
    "    else:\n",
    "        print(\"âœ… å…¨ã¦ã®æ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "        return True\n",
    "\n",
    "# ğŸ”„ å®Ÿé¨“è¨­å®šï¼ˆãƒ–ãƒ©ãƒ³ãƒé¸æŠï¼‰\n",
    "# ===============================================\n",
    "EXPERIMENT_BRANCH = \"main\"  # ã“ã“ã‚’å¤‰æ›´ã—ã¦ç•°ãªã‚‹ãƒ–ãƒ©ãƒ³ãƒã‚’ä½¿ç”¨\n",
    "# EXPERIMENT_BRANCH = \"feature/new-model-support\"    # æ–°æ©Ÿèƒ½ãƒ–ãƒ©ãƒ³ãƒ\n",
    "# EXPERIMENT_BRANCH = \"experiment/temperature-test\"  # å®Ÿé¨“ãƒ–ãƒ©ãƒ³ãƒ\n",
    "# EXPERIMENT_BRANCH = \"develop\"                       # é–‹ç™ºãƒ–ãƒ©ãƒ³ãƒ\n",
    "\n",
    "# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ\n",
    "print(\"ğŸš€ Google Colabç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "IN_COLAB = setup_colab_environment()\n",
    "\n",
    "# Git sparse checkoutã®å®Ÿè¡Œ\n",
    "success, commit_hash = git_sparse_checkout(EXPERIMENT_BRANCH)\n",
    "\n",
    "if success:\n",
    "    print(f\"\\nğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    install_dependencies()\n",
    "    \n",
    "    # ãƒ‘ã‚¹ã®è¿½åŠ ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆç”¨ï¼‰- Google Colabç’°å¢ƒå¯¾å¿œ\n",
    "    repo_path = os.path.abspath(\"my_sae\")\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.insert(0, repo_path)\n",
    "        print(f\"ğŸ“ Pythonãƒ‘ã‚¹ã«è¿½åŠ : {repo_path}\")\n",
    "    \n",
    "    # Colabç’°å¢ƒã§ã¯ /content/ ã‚‚ãƒ‘ã‚¹ã«è¿½åŠ \n",
    "    if IN_COLAB:\n",
    "        content_path = \"/content\"\n",
    "        content_repo_path = \"/content/my_sae\"\n",
    "        if content_path not in sys.path:\n",
    "            sys.path.insert(0, content_path)\n",
    "        if content_repo_path not in sys.path:\n",
    "            sys.path.insert(0, content_repo_path)\n",
    "        print(f\"ğŸ“ Colabç”¨ãƒ‘ã‚¹è¿½åŠ : {content_path}, {content_repo_path}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼\")\n",
    "    print(f\"ğŸ”– ä½¿ç”¨ãƒ–ãƒ©ãƒ³ãƒ: {EXPERIMENT_BRANCH}\")\n",
    "    print(f\"ğŸ“Œ ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥: {commit_hash}\")\n",
    "    print(\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ«ã§ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„\")\n",
    "    \n",
    "    # å®Ÿé¨“è¨˜éŒ²ç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°\n",
    "    EXPERIMENT_METADATA = {\n",
    "        'branch': EXPERIMENT_BRANCH,\n",
    "        'commit_hash': commit_hash,\n",
    "        'setup_time': __import__('datetime').datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "else:\n",
    "    print(f\"\\nâŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(f\"ğŸ’¡ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã¾ãŸã¯ãƒ–ãƒ©ãƒ³ãƒ'{EXPERIMENT_BRANCH}'ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45d8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨è¨­å®š\n",
    "try:\n",
    "    # ãƒ¡ã‚¤ãƒ³ã®åˆ†æã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from sycophancy_analyzer import SycophancyAnalyzer\n",
    "    \n",
    "    # è¨­å®šã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "    from config import (\n",
    "        GEMMA2B_TEST_CONFIG, GEMMA2B_PROD_CONFIG, GEMMA2B_MEMORY_OPTIMIZED_CONFIG,\n",
    "        LLAMA3_TEST_CONFIG, LLAMA3_MEMORY_OPTIMIZED_CONFIG,\n",
    "        TEST_CONFIG, DEFAULT_CONFIG,\n",
    "        get_auto_config\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†\")\n",
    "    print(\"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãªè¨­å®š:\")\n",
    "    print(\"   ğŸ”¸ GEMMA2B_TEST_CONFIG - Gemma-2B ãƒ†ã‚¹ãƒˆç”¨ï¼ˆå°‘æ•°ã‚µãƒ³ãƒ—ãƒ«ï¼‰\")\n",
    "    print(\"   ğŸ”¸ GEMMA2B_PROD_CONFIG - Gemma-2B æœ¬ç•ªç”¨ï¼ˆå¤§è¦æ¨¡åˆ†æï¼‰\") \n",
    "    print(\"   ğŸ”¸ GEMMA2B_MEMORY_OPTIMIZED_CONFIG - Gemma-2B ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆ\")\n",
    "    print(\"   ğŸ”¸ LLAMA3_TEST_CONFIG - Llama-3.2-1B ãƒ†ã‚¹ãƒˆç”¨\")\n",
    "    print(\"   ğŸ”¸ LLAMA3_MEMORY_OPTIMIZED_CONFIG - Llama-3.2-1B ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆ\")\n",
    "    print(\"   ğŸ”¸ TEST_CONFIG - GPT-2 ãƒ†ã‚¹ãƒˆç”¨\")\n",
    "    print(\"   ğŸ”¸ get_auto_config() - ç’°å¢ƒã«å¿œã˜ãŸè‡ªå‹•é¸æŠ\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ ä¸Šè¨˜ã®ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    raise\n",
    "\n",
    "# ğŸ¯ é«˜é€Ÿå®Ÿé¨“è¨­å®šï¼ˆã“ã“ã‚’å¤‰æ›´ã—ã¦å³åº§ã«å®Ÿé¨“æ¡ä»¶ã‚’å¤‰æ›´ï¼‰\n",
    "# ===============================================\n",
    "\n",
    "# ğŸ”§ å®Ÿé¨“è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºæ©Ÿèƒ½\n",
    "def create_custom_config(base_config, **kwargs):\n",
    "    \"\"\"è¨­å®šã‚’å‹•çš„ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹é–¢æ•°\"\"\"\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    custom_config = deepcopy(base_config)\n",
    "    \n",
    "    # å‹•çš„ã«è¨­å®šã‚’å¤‰æ›´\n",
    "    if 'sample_size' in kwargs:\n",
    "        custom_config.data.sample_size = kwargs['sample_size']\n",
    "    if 'model_name' in kwargs:\n",
    "        custom_config.model.name = kwargs['model_name']\n",
    "    if 'max_new_tokens' in kwargs:\n",
    "        custom_config.generation.max_new_tokens = kwargs['max_new_tokens']\n",
    "    if 'temperature' in kwargs:\n",
    "        custom_config.generation.temperature = kwargs['temperature']\n",
    "    if 'verbose' in kwargs:\n",
    "        custom_config.debug.verbose = kwargs['verbose']\n",
    "    \n",
    "    return custom_config\n",
    "\n",
    "# ğŸš€ å®Ÿé¨“è¨­å®šï¼ˆã“ã®éƒ¨åˆ†ã‚’å¤‰æ›´ã—ã¦å³åº§ã«å®Ÿé¨“æ¡ä»¶ã‚’å¤‰æ›´ï¼‰\n",
    "# ===============================================\n",
    "\n",
    "# åŸºæœ¬è¨­å®šã®é¸æŠ\n",
    "BASE_CONFIG = GEMMA2B_TEST_CONFIG  # ãƒ™ãƒ¼ã‚¹è¨­å®š\n",
    "\n",
    "# å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸Šæ›¸ãï¼ˆã“ã“ã‚’è‡ªç”±ã«å¤‰æ›´ï¼‰\n",
    "EXPERIMENT_PARAMS = {\n",
    "    'sample_size': 10,        # ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ1-1000ï¼‰\n",
    "    'temperature': 0.3,       # ç”Ÿæˆæ¸©åº¦ï¼ˆ0.0-1.0ï¼‰\n",
    "    'max_new_tokens': 3,      # æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    'verbose': True,          # è©³ç´°ãƒ­ã‚°å‡ºåŠ›\n",
    "}\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã®ä½œæˆ\n",
    "SELECTED_CONFIG = create_custom_config(BASE_CONFIG, **EXPERIMENT_PARAMS)\n",
    "\n",
    "print(f\"\\nğŸ¯ å®Ÿé¨“è¨­å®š:\")\n",
    "print(f\"   ğŸ“± ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {BASE_CONFIG.model.name}\")\n",
    "print(f\"   \udcca ã‚µãƒ³ãƒ—ãƒ«æ•°: {SELECTED_CONFIG.data.sample_size}\")\n",
    "print(f\"   ğŸŒ¡ï¸  æ¸©åº¦: {SELECTED_CONFIG.generation.temperature}\")\n",
    "print(f\"   \udd22 æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³: {SELECTED_CONFIG.generation.max_new_tokens}\")\n",
    "print(f\"   ğŸ” è©³ç´°ãƒ­ã‚°: {SELECTED_CONFIG.debug.verbose}\")\n",
    "print(f\"   ğŸ” SAE: {SELECTED_CONFIG.model.sae_release}\")\n",
    "print(f\"   ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {SELECTED_CONFIG.model.device}\")\n",
    "\n",
    "# ğŸ”„ ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯å®Ÿé¨“å¤‰æ›´ä¾‹\n",
    "print(f\"\\nğŸ”„ é«˜é€Ÿå®Ÿé¨“å¤‰æ›´ä¾‹:\")\n",
    "print(f\"# å¤§è¦æ¨¡å®Ÿé¨“ã«å¤‰æ›´\")\n",
    "print(f\"EXPERIMENT_PARAMS['sample_size'] = 100\")\n",
    "print(f\"\")\n",
    "print(f\"# é«˜æ¸©åº¦ç”Ÿæˆå®Ÿé¨“\")  \n",
    "print(f\"EXPERIMENT_PARAMS['temperature'] = 0.8\")\n",
    "print(f\"\")\n",
    "print(f\"# ã‚ˆã‚Šé•·ã„ç”Ÿæˆå®Ÿé¨“\")\n",
    "print(f\"EXPERIMENT_PARAMS['max_new_tokens'] = 10\")\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è¨­å®š\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "print(f\"\\nâœ… è¨­å®šå®Œäº† - æ¬¡ã®ã‚»ãƒ«ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "print(f\"ğŸ’¡ EXPERIMENT_PARAMSã‚’å¤‰æ›´ã—ã¦ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã™ã‚‹ã¨å³åº§ã«è¨­å®šãŒå¤‰æ›´ã•ã‚Œã¾ã™\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ LLMè¿åˆæ€§åˆ†æã®å®Ÿè¡Œ\n",
    "print(\"ğŸš€ LLMè¿åˆæ€§åˆ†æã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
    "experiment_metadata = {\n",
    "    'experiment_start': __import__('datetime').datetime.now().isoformat(),\n",
    "    'branch': EXPERIMENT_METADATA.get('branch', 'unknown') if 'EXPERIMENT_METADATA' in globals() else 'unknown',\n",
    "    'commit_hash': EXPERIMENT_METADATA.get('commit_hash', 'unknown') if 'EXPERIMENT_METADATA' in globals() else 'unknown',\n",
    "    'config_params': {\n",
    "        'model_name': SELECTED_CONFIG.model.name,\n",
    "        'sample_size': SELECTED_CONFIG.data.sample_size,\n",
    "        'temperature': SELECTED_CONFIG.generation.temperature,\n",
    "        'max_new_tokens': SELECTED_CONFIG.generation.max_new_tokens,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ğŸ“‹ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:\")\n",
    "print(f\"   ğŸŒ¿ ãƒ–ãƒ©ãƒ³ãƒ: {experiment_metadata['branch']}\")\n",
    "print(f\"   ğŸ“Œ ã‚³ãƒŸãƒƒãƒˆ: {experiment_metadata['commit_hash']}\")\n",
    "print(f\"   â° é–‹å§‹æ™‚åˆ»: {experiment_metadata['experiment_start']}\")\n",
    "print(f\"   ğŸ“Š è¨­å®š: {experiment_metadata['config_params']}\")\n",
    "\n",
    "try:\n",
    "    # åˆ†æå™¨ã®åˆæœŸåŒ–\n",
    "    print(f\"\\nğŸ”§ åˆ†æå™¨ã‚’åˆæœŸåŒ–ä¸­...\")\n",
    "    analyzer = SycophancyAnalyzer(SELECTED_CONFIG)\n",
    "    \n",
    "    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®è¡¨ç¤º\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"ğŸ’¾ åˆæœŸGPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "        experiment_metadata['initial_memory_gb'] = round(memory_used, 2)\n",
    "    \n",
    "    # å®Œå…¨åˆ†æã®å®Ÿè¡Œï¼ˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ + åˆ†æ + çµæœä¿å­˜ã‚’ä¸€æ‹¬å®Ÿè¡Œï¼‰\n",
    "    print(f\"\\nğŸ”„ å®Œå…¨åˆ†æã‚’å®Ÿè¡Œä¸­...\")\n",
    "    print(f\"   ğŸ“‹ ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...\")\n",
    "    \n",
    "    # å®Ÿè¡Œæ™‚é–“æ¸¬å®šé–‹å§‹\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³åˆ†æã®å®Ÿè¡Œ\n",
    "    analyzer.run_complete_analysis()\n",
    "    \n",
    "    # å®Ÿè¡Œæ™‚é–“æ¸¬å®šçµ‚äº†\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    experiment_metadata['execution_time_seconds'] = round(execution_time, 2)\n",
    "    experiment_metadata['experiment_end'] = __import__('datetime').datetime.now().isoformat()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 60)\n",
    "    print(\"âœ… åˆ†æå®Œäº†ï¼\")\n",
    "    print(f\"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’\")\n",
    "    \n",
    "    # çµæœã®ç°¡æ˜“è¡¨ç¤º\n",
    "    if hasattr(analyzer, 'analysis_results') and analyzer.analysis_results:\n",
    "        stats = analyzer.analysis_results\n",
    "        \n",
    "        # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«çµæœã‚’è¿½åŠ \n",
    "        experiment_metadata['results'] = {\n",
    "            'total_samples': stats.get('total_samples', 0),\n",
    "            'sycophancy_rate': stats.get('sycophancy_rate', 0),\n",
    "            'initial_accuracy': stats.get('initial_accuracy', 0),\n",
    "            'challenge_accuracy': stats.get('challenge_accuracy', 0),\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:\")\n",
    "        print(f\"   ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {stats.get('model_name', 'N/A')}\")\n",
    "        print(f\"   ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats.get('total_samples', 0)}\")\n",
    "        print(f\"   ğŸ“ˆ è¿åˆç‡: {stats.get('sycophancy_rate', 0):.1%}\")\n",
    "        print(f\"   ğŸ“ˆ åˆå›æ­£ç­”ç‡: {stats.get('initial_accuracy', 0):.1%}\")\n",
    "        print(f\"   ğŸ“ˆ ãƒãƒ£ãƒ¬ãƒ³ã‚¸å¾Œæ­£ç­”ç‡: {stats.get('challenge_accuracy', 0):.1%}\")\n",
    "    \n",
    "    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®æœ€çµ‚ç¢ºèª\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        print(f\"\\nğŸ’¾ æœ€çµ‚GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB\")\n",
    "        experiment_metadata['final_memory_gb'] = round(memory_used, 2)\n",
    "    \n",
    "    # å®Ÿé¨“ãƒ­ã‚°ã®ä¿å­˜\n",
    "    import json\n",
    "    experiment_log_file = f\"experiment_log_{experiment_metadata['experiment_start'][:19].replace(':', '-')}.json\"\n",
    "    with open(experiment_log_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(experiment_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ å®Ÿé¨“ãƒ­ã‚°ä¿å­˜: {experiment_log_file}\")\n",
    "    print(f\"ğŸ’¡ è©³ç´°ãªçµæœã¯æ¬¡ã®ã‚»ãƒ«ã§å¯è¦–åŒ–ã•ã‚Œã¾ã™\")\n",
    "    \n",
    "    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«çµæœã‚’ä¿å­˜ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰\n",
    "    analysis_results = analyzer.analysis_results if hasattr(analyzer, 'analysis_results') else {}\n",
    "    detailed_results = analyzer.results if hasattr(analyzer, 'results') else []\n",
    "    \n",
    "    # å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¿å­˜\n",
    "    current_experiment_metadata = experiment_metadata\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ åˆ†æå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(f\"\\nğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\")\n",
    "    print(f\"   1. GPU ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã—ã¦ã„ãªã„ã‹ç¢ºèª\")\n",
    "    print(f\"   2. ã‚ˆã‚Šå°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®è¨­å®šã‚’è©¦è¡Œ\")\n",
    "    print(f\"   3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦å†å®Ÿè¡Œ\")\n",
    "    print(f\"   4. ç•°ãªã‚‹ãƒ–ãƒ©ãƒ³ãƒã‚’è©¦è¡Œï¼ˆEXPERIMENT_BRANCHå¤‰æ›´ï¼‰\")\n",
    "    \n",
    "    # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¨˜éŒ²\n",
    "    experiment_metadata['error'] = str(e)\n",
    "    experiment_metadata['status'] = 'failed'\n",
    "    \n",
    "    # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    raise\n",
    "\n",
    "print(f\"\\nğŸ‰ åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œäº† - æ¬¡ã®ã‚»ãƒ«ã§çµæœã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3027af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š åˆ†æçµæœã®è©³ç´°å¯è¦–åŒ–\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# çµæœãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª\n",
    "try:\n",
    "    if 'analysis_results' in globals() and 'detailed_results' in globals():\n",
    "        stats = analysis_results\n",
    "        results = detailed_results\n",
    "        \n",
    "        print(\"ğŸ“ˆ åˆ†æçµæœã®å¯è¦–åŒ–ã‚’é–‹å§‹...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not stats or not results:\n",
    "            print(\"âŒ å¯è¦–åŒ–ç”¨ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™\")\n",
    "            print(\"ğŸ’¡ ä¸Šè¨˜ã®åˆ†æå®Ÿè¡Œã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        else:\n",
    "            # 1. åŸºæœ¬çµ±è¨ˆã®è¡¨ç¤º\n",
    "            print(f\"ğŸ“Š è©³ç´°çµ±è¨ˆ:\")\n",
    "            print(f\"   ğŸ¯ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {stats.get('model_name', 'N/A')}\")\n",
    "            print(f\"   ğŸ“ ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats.get('total_samples', 0)}\")\n",
    "            print(f\"   ğŸ”„ è¿åˆäº‹ä¾‹æ•°: {stats.get('sycophantic_responses', 0)}\")\n",
    "            print(f\"   ğŸ“ˆ è¿åˆç‡: {stats.get('sycophancy_rate', 0):.1%}\")\n",
    "            print(f\"   ğŸ“ˆ åˆå›æ­£ç­”ç‡: {stats.get('initial_accuracy', 0):.1%}\")\n",
    "            print(f\"   ğŸ“ˆ ãƒãƒ£ãƒ¬ãƒ³ã‚¸å¾Œæ­£ç­”ç‡: {stats.get('challenge_accuracy', 0):.1%}\")\n",
    "            \n",
    "            # 2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªå¯è¦–åŒ–ï¼ˆPlotlyï¼‰\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=['è¿åˆç‡åˆ†æ', 'æ­£ç­”ç‡æ¯”è¼ƒ', 'å›ç­”å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³', 'è¿åˆæ™‚ã®æ­£ç­”ç‡'],\n",
    "                specs=[[{\"type\": \"pie\"}, {\"type\": \"bar\"}],\n",
    "                       [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    "            )\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: è¿åˆç‡ï¼ˆãƒ‘ã‚¤ãƒãƒ£ãƒ¼ãƒˆï¼‰\n",
    "            sycophantic_count = stats.get('sycophantic_responses', 0)\n",
    "            consistent_count = stats.get('total_samples', 0) - sycophantic_count\n",
    "            \n",
    "            fig.add_trace(go.Pie(\n",
    "                labels=['ä¸€è²«ã—ãŸå›ç­”', 'è¿åˆçš„å›ç­”'],\n",
    "                values=[consistent_count, sycophantic_count],\n",
    "                hole=0.3\n",
    "            ), row=1, col=1)\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: æ­£ç­”ç‡æ¯”è¼ƒ\n",
    "            accuracies = [stats.get('initial_accuracy', 0), stats.get('challenge_accuracy', 0)]\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=['åˆå›å›ç­”', 'ãƒãƒ£ãƒ¬ãƒ³ã‚¸å¾Œ'],\n",
    "                y=accuracies,\n",
    "                text=[f'{acc:.1%}' for acc in accuracies],\n",
    "                textposition='auto'\n",
    "            ), row=1, col=2)\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: å›ç­”å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "            if results:\n",
    "                pattern_counts = {'æ­£è§£â†’æ­£è§£': 0, 'æ­£è§£â†’ä¸æ­£è§£': 0, 'ä¸æ­£è§£â†’æ­£è§£': 0, 'ä¸æ­£è§£â†’ä¸æ­£è§£': 0}\n",
    "                for result in results:\n",
    "                    initial_correct = result.get('is_initial_correct', False)\n",
    "                    challenge_correct = result.get('is_challenge_correct', False)\n",
    "                    \n",
    "                    if initial_correct and challenge_correct:\n",
    "                        pattern_counts['æ­£è§£â†’æ­£è§£'] += 1\n",
    "                    elif initial_correct and not challenge_correct:\n",
    "                        pattern_counts['æ­£è§£â†’ä¸æ­£è§£'] += 1\n",
    "                    elif not initial_correct and challenge_correct:\n",
    "                        pattern_counts['ä¸æ­£è§£â†’æ­£è§£'] += 1\n",
    "                    else:\n",
    "                        pattern_counts['ä¸æ­£è§£â†’ä¸æ­£è§£'] += 1\n",
    "                \n",
    "                fig.add_trace(go.Bar(\n",
    "                    x=list(pattern_counts.keys()),\n",
    "                    y=list(pattern_counts.values()),\n",
    "                    text=list(pattern_counts.values()),\n",
    "                    textposition='auto'\n",
    "                ), row=2, col=1)\n",
    "            \n",
    "            # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: è¿åˆæ™‚ã®æ­£ç­”ç‡è©³ç´°\n",
    "            if results:\n",
    "                sycophantic_cases = [r for r in results if r.get('is_sycophantic', False)]\n",
    "                if sycophantic_cases:\n",
    "                    syc_initial_correct = sum(1 for r in sycophantic_cases if r.get('is_initial_correct', False))\n",
    "                    syc_challenge_correct = sum(1 for r in sycophantic_cases if r.get('is_challenge_correct', False))\n",
    "                    syc_total = len(sycophantic_cases)\n",
    "                    \n",
    "                    syc_accuracies = [syc_initial_correct/syc_total, syc_challenge_correct/syc_total] if syc_total > 0 else [0, 0]\n",
    "                    \n",
    "                    fig.add_trace(go.Bar(\n",
    "                        x=['è¿åˆæ™‚åˆå›', 'è¿åˆæ™‚ãƒãƒ£ãƒ¬ãƒ³ã‚¸'],\n",
    "                        y=syc_accuracies,\n",
    "                        text=[f'{acc:.1%}' for acc in syc_accuracies],\n",
    "                        textposition='auto',\n",
    "                        marker_color='orange'\n",
    "                    ), row=2, col=2)\n",
    "            \n",
    "            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´\n",
    "            fig.update_layout(\n",
    "                title_text=f\"LLMè¿åˆæ€§åˆ†æçµæœ - {stats.get('model_name', 'Unknown Model')}\",\n",
    "                height=800,\n",
    "                showlegend=False\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # 3. å…·ä½“ä¾‹ã®è¡¨ç¤ºï¼ˆæœ€åˆã®3ä»¶ï¼‰\n",
    "            if results:\n",
    "                print(f\"\\nğŸ“ åˆ†æäº‹ä¾‹ï¼ˆæœ€åˆã®3ä»¶ï¼‰:\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for i, result in enumerate(results[:3]):\n",
    "                    print(f\"\\näº‹ä¾‹ {i+1}:\")\n",
    "                    print(f\"  è³ªå•: {result.get('question', 'N/A')[:60]}...\")\n",
    "                    print(f\"  æ­£ç­”: {result.get('correct_answer', 'N/A')}\")\n",
    "                    print(f\"  åˆå›å›ç­”: {result.get('initial_choice', 'N/A')} ({'æ­£è§£' if result.get('is_initial_correct') else 'ä¸æ­£è§£'})\")\n",
    "                    print(f\"  ãƒãƒ£ãƒ¬ãƒ³ã‚¸å›ç­”: {result.get('challenge_choice', 'N/A')} ({'æ­£è§£' if result.get('is_challenge_correct') else 'ä¸æ­£è§£'})\")\n",
    "                    print(f\"  è¿åˆåˆ¤å®š: {'ã‚ã‚Š' if result.get('is_sycophantic') else 'ãªã—'}\")\n",
    "            \n",
    "            # 4. çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜æƒ…å ±\n",
    "            print(f\"\\nğŸ’¾ çµæœä¿å­˜:\")\n",
    "            print(f\"   ğŸ“ çµæœãƒ•ã‚¡ã‚¤ãƒ«ã¯ 'results/' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ\")\n",
    "            print(f\"   ğŸ“Š å¯è¦–åŒ–å›³è¡¨ã¯ 'plots/' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ\")\n",
    "            \n",
    "            print(f\"\\n\" + \"=\" * 50)\n",
    "            print(\"âœ… å¯è¦–åŒ–å®Œäº†ï¼\")\n",
    "    \n",
    "    else:\n",
    "        print(\"âŒ åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "        print(\"ğŸ’¡ ä¸Šè¨˜ã®åˆ†æå®Ÿè¡Œã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    print(\"ğŸ’¡ åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
