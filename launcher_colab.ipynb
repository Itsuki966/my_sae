{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ae9ba82",
   "metadata": {},
   "source": [
    "# ğŸš€ LLMè¿åˆæ€§åˆ†æãƒ©ãƒ³ãƒãƒ£ãƒ¼ - Google Colabç‰ˆ\n",
    "\n",
    "**ğŸ“‹ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„**\n",
    "\n",
    "Google Colabç’°å¢ƒã§LLMã®è¿åˆæ€§ï¼ˆSycophancyï¼‰åˆ†æã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒ³ãƒãƒ£ãƒ¼ã§ã™ã€‚\n",
    "\n",
    "## ğŸ¯ ä¸»è¦ãªæ”¹å–„ç‚¹\n",
    "\n",
    "1. **ç’°å¢ƒæ§‹ç¯‰ã¨å®Ÿé¨“å®Ÿè¡Œã®åˆ†é›¢**: å•é¡Œã®åˆ‡ã‚Šåˆ†ã‘ãŒå®¹æ˜“\n",
    "2. **æ®µéšçš„ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: `requirements.txt`ã®ãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—å•é¡Œã‚’è§£æ±º\n",
    "3. **Git sparse-checkout**: å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿é«˜é€Ÿå–å¾—\n",
    "4. **è‡ªå‹•å®Ÿé¨“å®Ÿè¡Œ**: nbconvertã«ã‚ˆã‚‹å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯è‡ªå‹•å®Ÿè¡Œ\n",
    "5. **åŒ…æ‹¬çš„ãƒ‡ãƒãƒƒã‚°æ”¯æ´**: å•é¡Œç™ºç”Ÿæ™‚ã®è¿…é€Ÿãªç‰¹å®šã¨è§£æ±º\n",
    "\n",
    "## ğŸ”§ ä½¿ç”¨æ–¹æ³•\n",
    "\n",
    "1. **ä¸Šã‹ã‚‰é †ç•ªã«å„ã‚»ãƒ«ã‚’å®Ÿè¡Œ**ã—ã¦ãã ã•ã„\n",
    "2. **ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ**ã¯è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å‚ç…§\n",
    "3. **å®Ÿé¨“è¨­å®šã‚’å¤‰æ›´ã—ãŸã„å ´åˆ**ã¯å¯¾å¿œã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è¨­å®šã‚’èª¿æ•´\n",
    "\n",
    "## âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …\n",
    "\n",
    "- ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯**Google Colabç’°å¢ƒå°‚ç”¨**ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™\n",
    "- å„ã‚»ãƒ«ã¯**ä¸Šã‹ã‚‰é †ç•ªã«å®Ÿè¡Œ**ã—ã¦ãã ã•ã„\n",
    "- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’äº‹å‰ã«Google Driveã«é…ç½®**ã—ã¦ãã ã•ã„ï¼ˆè©³ç´°ã¯ä¸‹è¨˜å‚ç…§ï¼‰\n",
    "- **å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ç„¦ã‚‰ãš**ã€è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦ãã ã•ã„\n",
    "\n",
    "## ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ï¼ˆå®Ÿè¡Œå‰ã«å¿…è¦ï¼‰\n",
    "\n",
    "**å®Ÿè¡Œå‰ã«ä»¥ä¸‹ã®æº–å‚™ã‚’å®Œäº†ã—ã¦ãã ã•ã„ï¼š**\n",
    "\n",
    "1. **Google Driveã«`eval_dataset`ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ**\n",
    "2. **ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼š**\n",
    "   - `are_you_sure.jsonl` (ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ - å¿…é ˆ)\n",
    "   - `answer.jsonl`, `feedback.jsonl`, `few_shot_examples.jsonl` (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)\n",
    "3. **æ¨å¥¨é…ç½®å ´æ‰€ï¼š** `/content/drive/MyDrive/eval_dataset/`\n",
    "\n",
    "**æ³¨æ„ï¼š** ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒãªã„å ´åˆã€ã‚»ãƒ«2.5ã§è©³ç´°ãªé…ç½®æ‰‹é †ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æº–å‚™ãŒã§ããŸã‚‰ä¸‹ã®ã‚»ãƒ«ã‹ã‚‰å®Ÿè¡Œã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e78ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ã‚»ãƒ«1: ç’°å¢ƒæ§‹ç¯‰ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆ\n",
    "# ==============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# ğŸ“‹ å®Ÿé¨“è¨­å®šï¼ˆã“ã“ã§ä¸€å…ƒç®¡ç†ï¼‰\n",
    "# ==============================================\n",
    "REPO_URL = \"https://github.com/Itsuki966/my_sae.git\"\n",
    "REPO_DIR = \"my_sae\"\n",
    "EXPERIMENT_BRANCH = \"main\"  # ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ©ãƒ³ãƒ\n",
    "TARGET_NOTEBOOK = \"sycophancy_analysis_colab.ipynb\"\n",
    "\n",
    "# å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "EXPERIMENT_SESSION = {\n",
    "    'session_id': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    'start_time': datetime.now().isoformat(),\n",
    "    'branch': EXPERIMENT_BRANCH,\n",
    "    'colab_detected': False,\n",
    "    'gpu_available': False,\n",
    "    'environment_status': 'initializing'\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ LLMè¿åˆæ€§åˆ†æãƒ©ãƒ³ãƒãƒ£ãƒ¼ - ç’°å¢ƒæ§‹ç¯‰é–‹å§‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Google Colabç’°å¢ƒã®æ¤œå‡º\n",
    "def detect_colab_environment():\n",
    "    \"\"\"Google Colabç’°å¢ƒã‚’æ¤œå‡ºã—ã€åŸºæœ¬æƒ…å ±ã‚’è¡¨ç¤º\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"âœ… Google Colabç’°å¢ƒã‚’æ¤œå‡º\")\n",
    "        EXPERIMENT_SESSION['colab_detected'] = True\n",
    "        \n",
    "        # ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æƒ…å ±ã®è¡¨ç¤º\n",
    "        print(\"ğŸ“Š Colabç’°å¢ƒæƒ…å ±:\")\n",
    "        print(f\"   ğŸ”– ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {EXPERIMENT_SESSION['session_id']}\")\n",
    "        print(f\"   ğŸ• é–‹å§‹æ™‚åˆ»: {EXPERIMENT_SESSION['start_time']}\")\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Ÿè¡Œã‚’æ¤œå‡º\")\n",
    "        print(\"ğŸ’¡ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯Google Colabç”¨ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™\")\n",
    "        EXPERIMENT_SESSION['colab_detected'] = False\n",
    "        return False\n",
    "\n",
    "# GPUç’°å¢ƒã®ç¢ºèª\n",
    "def check_gpu_environment():\n",
    "    \"\"\"GPUç’°å¢ƒã®ç¢ºèªã¨è¡¨ç¤º\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            \n",
    "            print(\"ğŸš€ GPUç’°å¢ƒ:\")\n",
    "            print(f\"   ğŸ“± GPUå: {gpu_name}\")\n",
    "            print(f\"   ğŸ’¾ VRAM: {gpu_memory:.1f}GB\")\n",
    "            print(f\"   ğŸ”¢ GPUæ•°: {gpu_count}\")\n",
    "            \n",
    "            EXPERIMENT_SESSION['gpu_available'] = True\n",
    "            EXPERIMENT_SESSION['gpu_name'] = gpu_name\n",
    "            EXPERIMENT_SESSION['gpu_memory_gb'] = round(gpu_memory, 1)\n",
    "            \n",
    "            # T4 GPUç‰¹åŒ–ã®æœ€é©åŒ–ãƒ’ãƒ³ãƒˆ\n",
    "            if \"T4\" in gpu_name:\n",
    "                print(\"ğŸ¯ T4 GPUæ¤œå‡º: Gemma-2Bãƒ¢ãƒ‡ãƒ«ãŒæ¨å¥¨ã•ã‚Œã¾ã™\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ GPUåˆ©ç”¨ä¸å¯ - CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™\")\n",
    "            print(\"âš ï¸ å®Ÿè¡Œæ™‚é–“ãŒå¤§å¹…ã«å¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "            EXPERIMENT_SESSION['gpu_available'] = False\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âŒ PyTorchãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        return False\n",
    "\n",
    "# åŸºæœ¬ç’°å¢ƒå¤‰æ•°ã®è¨­å®š\n",
    "def setup_environment_variables():\n",
    "    \"\"\"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã®ç’°å¢ƒå¤‰æ•°è¨­å®š\"\"\"\n",
    "    print(\"ğŸ”§ ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šä¸­...\")\n",
    "    \n",
    "    # PyTorchãƒ¡ãƒ¢ãƒªç®¡ç†æœ€é©åŒ–  \n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    \n",
    "    # è­¦å‘Šã®æŠ‘åˆ¶\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    print(\"âœ… ç’°å¢ƒå¤‰æ•°è¨­å®šå®Œäº†\")\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ç’°å¢ƒæ¤œå‡ºã®å®Ÿè¡Œ\n",
    "print(\"ğŸ” ç’°å¢ƒæ¤œå‡ºã‚’é–‹å§‹...\")\n",
    "\n",
    "colab_detected = detect_colab_environment()\n",
    "gpu_available = check_gpu_environment()\n",
    "setup_environment_variables()\n",
    "\n",
    "EXPERIMENT_SESSION['environment_status'] = 'configured'\n",
    "\n",
    "print(f\"\\nğŸ“‹ ç’°å¢ƒæ§‹ç¯‰ã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"   ğŸŒ Colabç’°å¢ƒ: {'âœ…' if colab_detected else 'âŒ'}\")\n",
    "print(f\"   ğŸš€ GPUåˆ©ç”¨: {'âœ…' if gpu_available else 'âŒ'}\")\n",
    "print(f\"   ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {REPO_DIR}\")\n",
    "print(f\"   ğŸŒ¿ å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ: {EXPERIMENT_BRANCH}\")\n",
    "print(f\"   ğŸ““ å®Ÿè¡Œäºˆå®šãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: {TARGET_NOTEBOOK}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ æ¬¡ã®ã‚»ãƒ« (ã‚»ãƒ«2) ã§ãƒªãƒã‚¸ãƒˆãƒªå–å¾—ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d6757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“‚ ã‚»ãƒ«2: ãƒªãƒã‚¸ãƒˆãƒªç®¡ç†ã¨sparse-checkoutå®Ÿè£…\n",
    "# ==============================================\n",
    "\n",
    "def execute_git_sparse_checkout():\n",
    "    \"\"\"Git sparse-checkoutã‚’ä½¿ç”¨ã—ã¦ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å–å¾—\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‚ Git sparse-checkoutã‚’é–‹å§‹...\")\n",
    "    \n",
    "    # æ—¢å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "    if os.path.exists(REPO_DIR):\n",
    "        print(f\"ğŸ§¹ æ—¢å­˜ã®{REPO_DIR}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤ä¸­...\")\n",
    "        !rm -rf {REPO_DIR}\n",
    "    \n",
    "    try:\n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—1: éª¨æ ¼ã®ã¿ã‚¯ãƒ­ãƒ¼ãƒ³ï¼ˆé«˜é€Ÿï¼‰\n",
    "        print(\"ğŸ“¥ ãƒªãƒã‚¸ãƒˆãƒªéª¨æ ¼ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ä¸­...\")\n",
    "        !git clone --filter=blob:none --no-checkout -b {EXPERIMENT_BRANCH} {REPO_URL} {REPO_DIR}\n",
    "        \n",
    "        # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(REPO_DIR)\n",
    "        \n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—2: sparse-checkoutã®åˆæœŸåŒ–\n",
    "        print(\"ğŸ”§ sparse-checkoutã‚’åˆæœŸåŒ–ä¸­...\")\n",
    "        !git sparse-checkout init --cone\n",
    "        \n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—3: å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®æŒ‡å®šï¼ˆeval_datasetã¯Google Driveã‹ã‚‰å–å¾—ï¼‰\n",
    "        required_files = [\n",
    "            \"sycophancy_analyzer.py\",\n",
    "            \"config.py\", \n",
    "            \"memory_optimizer.py\",\n",
    "            \"sycophancy_analysis_colab.ipynb\",\n",
    "            \"requirements.txt\",\n",
    "            \"pyproject.toml\"\n",
    "        ]\n",
    "        \n",
    "        print(\"ğŸ“‹ å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šä¸­...\")\n",
    "        for file in required_files:\n",
    "            print(f\"   ğŸ”¸ {file}\")\n",
    "        \n",
    "        # sparse-checkoutã®è¨­å®š\n",
    "        files_spec = \" \".join(required_files)\n",
    "        !git sparse-checkout set {files_spec}\n",
    "        \n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—4: å®Ÿéš›ã®ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆå®Ÿè¡Œ\n",
    "        print(\"ğŸ“‹ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã‚¢ã‚¦ãƒˆä¸­...\")\n",
    "        !git checkout {EXPERIMENT_BRANCH}\n",
    "        \n",
    "        # ã‚³ãƒŸãƒƒãƒˆæƒ…å ±ã®å–å¾—\n",
    "        commit_info = !git log --oneline -1\n",
    "        commit_hash = !git rev-parse --short HEAD\n",
    "        \n",
    "        EXPERIMENT_SESSION['commit_hash'] = commit_hash[0] if commit_hash else 'unknown'\n",
    "        EXPERIMENT_SESSION['commit_info'] = commit_info[0] if commit_info else 'unknown'\n",
    "        \n",
    "        # å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã‚‹\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        # å–å¾—çµæœã®ç¢ºèª\n",
    "        print(f\"\\nâœ… ãƒªãƒã‚¸ãƒˆãƒªå–å¾—å®Œäº†:\")\n",
    "        print(f\"   ğŸ“Œ ã‚³ãƒŸãƒƒãƒˆ: {EXPERIMENT_SESSION['commit_hash']}\")\n",
    "        print(f\"   ğŸ“ {EXPERIMENT_SESSION['commit_info']}\")\n",
    "        \n",
    "        # å–å¾—ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "        print(f\"\\nğŸ“ å–å¾—ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:\")\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(REPO_DIR, file)\n",
    "            if os.path.exists(file_path):\n",
    "                if os.path.isfile(file_path):\n",
    "                    size_kb = os.path.getsize(file_path) / 1024\n",
    "                    print(f\"   âœ… {file} ({size_kb:.1f}KB)\")\n",
    "                else:\n",
    "                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å ´åˆ\n",
    "                    try:\n",
    "                        file_count = len([f for f in os.listdir(file_path) \n",
    "                                        if os.path.isfile(os.path.join(file_path, f))])\n",
    "                        print(f\"   âœ… {file}/ ({file_count}ãƒ•ã‚¡ã‚¤ãƒ«)\")\n",
    "                    except:\n",
    "                        print(f\"   âœ… {file}/ (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)\")\n",
    "            else:\n",
    "                print(f\"   âŒ {file} (è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)\")\n",
    "        \n",
    "        # ãƒ‘ã‚¹ã«ãƒªãƒã‚¸ãƒˆãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ \n",
    "        repo_path = os.path.abspath(REPO_DIR)\n",
    "        if repo_path not in sys.path:\n",
    "            sys.path.insert(0, repo_path)\n",
    "            print(f\"ğŸ“ Pythonãƒ‘ã‚¹ã«è¿½åŠ : {repo_path}\")\n",
    "        \n",
    "        EXPERIMENT_SESSION['repository_status'] = 'ready'\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒªãƒã‚¸ãƒˆãƒªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(f\"ğŸ’¡ ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\")\n",
    "        print(f\"   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶š\")\n",
    "        print(f\"   - ãƒ–ãƒ©ãƒ³ãƒ'{EXPERIMENT_BRANCH}'ã®å­˜åœ¨\")\n",
    "        print(f\"   - ãƒªãƒã‚¸ãƒˆãƒªURL: {REPO_URL}\")\n",
    "        \n",
    "        EXPERIMENT_SESSION['repository_status'] = 'failed'\n",
    "        EXPERIMENT_SESSION['repository_error'] = str(e)\n",
    "        return False\n",
    "\n",
    "def check_repository_integrity():\n",
    "    \"\"\"å–å¾—ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã®æ•´åˆæ€§ç¢ºèª\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ãƒªãƒã‚¸ãƒˆãƒªæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...\")\n",
    "    \n",
    "    critical_files = [\"sycophancy_analyzer.py\", \"config.py\"]\n",
    "    missing_files = []\n",
    "    \n",
    "    for file in critical_files:\n",
    "        file_path = os.path.join(REPO_DIR, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"âŒ é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³:\")\n",
    "        for file in missing_files:\n",
    "            print(f\"   - {file}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"âœ… é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ç¢ºèªå®Œäº†\")\n",
    "        return True\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ãƒªãƒã‚¸ãƒˆãƒªå–å¾—å‡¦ç†ã®å®Ÿè¡Œ\n",
    "print(\"ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒªç®¡ç†ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "success = execute_git_sparse_checkout()\n",
    "\n",
    "if success:\n",
    "    integrity_ok = check_repository_integrity()\n",
    "    \n",
    "    if integrity_ok:\n",
    "        print(f\"\\nğŸ‰ ãƒªãƒã‚¸ãƒˆãƒªæº–å‚™å®Œäº†!\")\n",
    "        print(f\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ« (ã‚»ãƒ«3) ã§ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ ãƒªãƒã‚¸ãƒˆãƒªã«å•é¡ŒãŒã‚ã‚Šã¾ã™\")\n",
    "        print(f\"ğŸ’¡ ã‚»ãƒ«5ã®ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦å•é¡Œã‚’ç‰¹å®šã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(f\"\\nâŒ ãƒªãƒã‚¸ãƒˆãƒªå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(f\"ğŸ’¡ ã‚»ãƒ«5ã®ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ ã‚»ãƒ«2.5: Google Driveãƒã‚¦ãƒ³ãƒˆã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™\n",
    "# ==============================================\n",
    "\n",
    "def mount_google_drive():\n",
    "    \"\"\"Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æº–å‚™\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“ Google Driveãƒã‚¦ãƒ³ãƒˆã‚’é–‹å§‹...\")\n",
    "    \n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        \n",
    "        # Google Driveãƒã‚¦ãƒ³ãƒˆå®Ÿè¡Œ\n",
    "        print(\"ğŸ”’ Googleèªè¨¼ã‚’é–‹å§‹...\")\n",
    "        print(\"ğŸ’¡ ãƒ–ãƒ©ã‚¦ã‚¶ã§Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # ãƒã‚¦ãƒ³ãƒˆæˆåŠŸç¢ºèª\n",
    "        drive_path = '/content/drive/MyDrive'\n",
    "        if os.path.exists(drive_path):\n",
    "            print(\"âœ… Google Driveãƒã‚¦ãƒ³ãƒˆæˆåŠŸ\")\n",
    "            \n",
    "            # ãƒ‰ãƒ©ã‚¤ãƒ–å†…å®¹ã®ç¢ºèªï¼ˆæœ€åˆã®ãƒ¬ãƒ™ãƒ«ã®ã¿ï¼‰\n",
    "            try:\n",
    "                drive_contents = os.listdir(drive_path)\n",
    "                print(f\"ğŸ“‹ ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–å†…å®¹ ({len(drive_contents)}é …ç›®):\")\n",
    "                for item in drive_contents[:10]:  # æœ€åˆã®10é …ç›®ã®ã¿è¡¨ç¤º\n",
    "                    item_path = os.path.join(drive_path, item)\n",
    "                    if os.path.isdir(item_path):\n",
    "                        print(f\"   ğŸ“ {item}/\")\n",
    "                    else:\n",
    "                        print(f\"   ğŸ“„ {item}\")\n",
    "                if len(drive_contents) > 10:\n",
    "                    print(f\"   ... ãŠã‚ˆã³ {len(drive_contents) - 10} å€‹ã®è¿½åŠ é …ç›®\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ãƒ‰ãƒ©ã‚¤ãƒ–å†…å®¹ã®ç¢ºèªã«å¤±æ•—: {e}\")\n",
    "            \n",
    "            EXPERIMENT_SESSION['google_drive_mounted'] = True\n",
    "            EXPERIMENT_SESSION['drive_path'] = drive_path\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ Google Driveãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âŒ google.colabãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\")\n",
    "        print(\"ğŸ’¡ ã“ã®ã‚»ãƒ«ã¯Google Colabç’°å¢ƒã§ã®ã¿å‹•ä½œã—ã¾ã™\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Google Driveãƒã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_dataset_from_drive():\n",
    "    \"\"\"Google Driveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨­å®š\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’é–‹å§‹...\")\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¨å¥¨ãƒ‘ã‚¹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª¿æ•´å¯èƒ½ï¼‰\n",
    "    possible_dataset_paths = [\n",
    "        '/content/drive/MyDrive/sae_datasets/eval_dataset',\n",
    "        '/content/drive/MyDrive/research/sae/eval_dataset', \n",
    "        '/content/drive/MyDrive/eval_dataset',\n",
    "        '/content/drive/MyDrive/Colab Notebooks/eval_dataset',\n",
    "        '/content/drive/MyDrive/sae/eval_dataset'\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” ä»¥ä¸‹ã®ãƒ‘ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ¤œç´¢ä¸­:\")\n",
    "    for path in possible_dataset_paths:\n",
    "        print(f\"   ğŸ“ {path}\")\n",
    "    \n",
    "    dataset_path = None\n",
    "    found_files = []\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œç´¢\n",
    "    for candidate_path in possible_dataset_paths:\n",
    "        if os.path.exists(candidate_path):\n",
    "            print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç™ºè¦‹: {candidate_path}\")\n",
    "            \n",
    "            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã®ç¢ºèª\n",
    "            try:\n",
    "                files = [f for f in os.listdir(candidate_path) \n",
    "                        if f.endswith('.jsonl') or f.endswith('.json')]\n",
    "                if files:\n",
    "                    dataset_path = candidate_path\n",
    "                    found_files = files\n",
    "                    print(f\"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« ({len(files)}å€‹):\")\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(candidate_path, file)\n",
    "                        if os.path.exists(file_path):\n",
    "                            size_kb = os.path.getsize(file_path) / 1024\n",
    "                            print(f\"   ğŸ“„ {file} ({size_kb:.1f}KB)\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not dataset_path:\n",
    "        print(\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ\")\n",
    "        print(\"\\nğŸ’¡ æ‰‹å‹•ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é…ç½®ã—ã¦ãã ã•ã„:\")\n",
    "        print(\"   1. Google Driveã« 'eval_dataset' ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ\")\n",
    "        print(\"   2. ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰:\")\n",
    "        print(\"      - are_you_sure.jsonl (ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ)\")\n",
    "        print(\"      - answer.jsonl, feedback.jsonl, few_shot_examples.jsonl\")\n",
    "        print(\"   3. æ¨å¥¨ãƒ‘ã‚¹: /content/drive/MyDrive/eval_dataset/\")\n",
    "        print(\"\\nğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé…ç½®å¾Œã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        \n",
    "        EXPERIMENT_SESSION['dataset_status'] = 'not_found'\n",
    "        return False\n",
    "    \n",
    "    # ãƒªãƒã‚¸ãƒˆãƒªå†…ã®eval_datasetã«ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "    repo_dataset_path = os.path.join(REPO_DIR, 'eval_dataset')\n",
    "    \n",
    "    try:\n",
    "        # æ—¢å­˜ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚„ãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°å‰Šé™¤\n",
    "        if os.path.exists(repo_dataset_path):\n",
    "            if os.path.islink(repo_dataset_path):\n",
    "                os.unlink(repo_dataset_path)\n",
    "            else:\n",
    "                import shutil\n",
    "                shutil.rmtree(repo_dataset_path)\n",
    "        \n",
    "        # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "        os.symlink(dataset_path, repo_dataset_path)\n",
    "        \n",
    "        print(f\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªãƒ³ã‚¯ä½œæˆæˆåŠŸ:\")\n",
    "        print(f\"   ğŸ“‚ {repo_dataset_path} -> {dataset_path}\")\n",
    "        \n",
    "        # ãƒªãƒ³ã‚¯ã®æ¤œè¨¼\n",
    "        if os.path.exists(repo_dataset_path):\n",
    "            linked_files = os.listdir(repo_dataset_path)\n",
    "            print(f\"ğŸ”— ãƒªãƒ³ã‚¯ç¢ºèª: {len(linked_files)}ãƒ•ã‚¡ã‚¤ãƒ«åˆ©ç”¨å¯èƒ½\")\n",
    "            \n",
    "            EXPERIMENT_SESSION['dataset_status'] = 'ready'\n",
    "            EXPERIMENT_SESSION['dataset_path'] = dataset_path\n",
    "            EXPERIMENT_SESSION['dataset_files'] = found_files\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã®ä½œæˆã«å¤±æ•—\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒªãƒ³ã‚¯ä½œæˆã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        print(\"ğŸ’¡ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥ã‚³ãƒ”ãƒ¼ã‚’è©¦è¡Œä¸­...\")\n",
    "        \n",
    "        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ã‚³ãƒ”ãƒ¼\n",
    "        try:\n",
    "            import shutil\n",
    "            os.makedirs(repo_dataset_path, exist_ok=True)\n",
    "            \n",
    "            copied_files = 0\n",
    "            for file in found_files:\n",
    "                src = os.path.join(dataset_path, file)\n",
    "                dst = os.path.join(repo_dataset_path, file)\n",
    "                shutil.copy2(src, dst)\n",
    "                copied_files += 1\n",
    "                print(f\"   ğŸ“„ {file} ã‚’ã‚³ãƒ”ãƒ¼\")\n",
    "            \n",
    "            print(f\"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: {copied_files}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼\")\n",
    "            \n",
    "            EXPERIMENT_SESSION['dataset_status'] = 'ready_copied'\n",
    "            EXPERIMENT_SESSION['dataset_path'] = repo_dataset_path\n",
    "            EXPERIMENT_SESSION['dataset_files'] = found_files\n",
    "            return True\n",
    "            \n",
    "        except Exception as copy_error:\n",
    "            print(f\"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚³ãƒ”ãƒ¼ã‚‚å¤±æ•—: {copy_error}\")\n",
    "            EXPERIMENT_SESSION['dataset_status'] = 'failed'\n",
    "            return False\n",
    "\n",
    "def verify_dataset_integrity():\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ•´åˆæ€§ç¢ºèª\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...\")\n",
    "    \n",
    "    repo_dataset_path = os.path.join(REPO_DIR, 'eval_dataset')\n",
    "    \n",
    "    if not os.path.exists(repo_dataset_path):\n",
    "        print(\"âŒ eval_datasetãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“\")\n",
    "        return False\n",
    "    \n",
    "    # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    critical_files = ['are_you_sure.jsonl']  # æœ€é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "    optional_files = ['answer.jsonl', 'feedback.jsonl', 'few_shot_examples.jsonl']\n",
    "    \n",
    "    missing_critical = []\n",
    "    missing_optional = []\n",
    "    \n",
    "    for file in critical_files:\n",
    "        file_path = os.path.join(repo_dataset_path, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_critical.append(file)\n",
    "        else:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚µãƒ³ãƒ—ãƒ«æ•°ã®ç¢ºèª\n",
    "            try:\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"âœ… {file} ({size_mb:.1f}MB)\")\n",
    "                \n",
    "                # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ç¢ºèª\n",
    "                if file.endswith('.jsonl'):\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        lines = sum(1 for line in f if line.strip())\n",
    "                    print(f\"   ğŸ“Š {lines}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ {file} ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    for file in optional_files:\n",
    "        file_path = os.path.join(repo_dataset_path, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            missing_optional.append(file)\n",
    "        else:\n",
    "            size_kb = os.path.getsize(file_path) / 1024\n",
    "            print(f\"âœ… {file} ({size_kb:.1f}KB)\")\n",
    "    \n",
    "    if missing_critical:\n",
    "        print(f\"âŒ é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³: {', '.join(missing_critical)}\")\n",
    "        return False\n",
    "    \n",
    "    if missing_optional:\n",
    "        print(f\"âš ï¸ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸è¶³: {', '.join(missing_optional)}\")\n",
    "        print(\"ğŸ’¡ ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªãã¦ã‚‚åŸºæœ¬çš„ãªåˆ†æã¯å®Ÿè¡Œå¯èƒ½ã§ã™\")\n",
    "    \n",
    "    print(\"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•´åˆæ€§ç¢ºèªå®Œäº†\")\n",
    "    return True\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å‡¦ç†\n",
    "print(\"ğŸ“ Google Driveãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Google Driveã®ãƒã‚¦ãƒ³ãƒˆ\n",
    "mount_success = mount_google_drive()\n",
    "\n",
    "if mount_success:\n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¨­å®š\n",
    "    dataset_success = setup_dataset_from_drive()\n",
    "    \n",
    "    if dataset_success:\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•´åˆæ€§ç¢ºèª\n",
    "        integrity_ok = verify_dataset_integrity()\n",
    "        \n",
    "        if integrity_ok:\n",
    "            print(f\"\\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†!\")\n",
    "            print(f\"ğŸ“Š åˆ†æã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {EXPERIMENT_SESSION.get('dataset_path', 'unknown')}\")\n",
    "            print(f\"ğŸ“‹ åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«: {len(EXPERIMENT_SESSION.get('dataset_files', []))}å€‹\")\n",
    "            print(f\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ« (ã‚»ãƒ«3) ã§ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™\")\n",
    "            print(f\"ğŸ’¡ ä¸Šè¨˜ã®æŒ‡ç¤ºã«å¾“ã£ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "        print(f\"ğŸ’¡ ä¸Šè¨˜ã®æŒ‡ç¤ºã«å¾“ã£ã¦Google Driveã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é…ç½®ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Google Driveãƒã‚¦ãƒ³ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(f\"ğŸ’¡ Colabã®èªè¨¼ç”»é¢ã§é©åˆ‡ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4397029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ ã‚»ãƒ«3: æ®µéšçš„ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "# ==============================================\n",
    "\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def install_package_safely(package_name, description=\"\", timeout=300):\n",
    "    \"\"\"å€‹åˆ¥ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®å®‰å…¨ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    print(f\"ğŸ“¦ {description or package_name} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # quietãƒ¢ãƒ¼ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-q', package_name], \n",
    "            timeout=timeout,\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"   âœ… {package_name} ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"   âŒ {package_name} - ã‚¨ãƒ©ãƒ¼: {result.stderr[:100]}...\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   â° {package_name} - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout}s)\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {package_name} - ä¾‹å¤–: {e}\")\n",
    "        return False\n",
    "\n",
    "def install_core_dependencies():\n",
    "    \"\"\"ã‚³ã‚¢ä¾å­˜é–¢ä¿‚ã®æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ ã‚³ã‚¢ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«é–‹å§‹...\")\n",
    "    \n",
    "    # ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "    print(\"\\nğŸ“‹ ãƒ•ã‚§ãƒ¼ã‚º1: åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\")\n",
    "    basic_packages = [\n",
    "        (\"torch\", \"PyTorch (GPUå¯¾å¿œ)\"),\n",
    "        (\"transformers\", \"HuggingFace Transformers\"),\n",
    "        (\"accelerate\", \"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\"),\n",
    "    ]\n",
    "    \n",
    "    failed_basic = []\n",
    "    for package, desc in basic_packages:\n",
    "        if not install_package_safely(package, desc, timeout=600):  # 10åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "            failed_basic.append(package)\n",
    "    \n",
    "    if failed_basic:\n",
    "        print(f\"âš ï¸ åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—: {', '.join(failed_basic)}\")\n",
    "        return False\n",
    "    \n",
    "    # ãƒ•ã‚§ãƒ¼ã‚º2: åˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "    print(\"\\nğŸ“‹ ãƒ•ã‚§ãƒ¼ã‚º2: åˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒª\")\n",
    "    analysis_packages = [\n",
    "        (\"plotly\", \"å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\"),\n",
    "        (\"tqdm\", \"ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼\"),\n",
    "        (\"numpy\", \"æ•°å€¤è¨ˆç®—\"),\n",
    "        (\"pandas\", \"ãƒ‡ãƒ¼ã‚¿å‡¦ç†\")\n",
    "    ]\n",
    "    \n",
    "    failed_analysis = []\n",
    "    for package, desc in analysis_packages:\n",
    "        if not install_package_safely(package, desc, timeout=300):  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "            failed_analysis.append(package)\n",
    "    \n",
    "    # ãƒ•ã‚§ãƒ¼ã‚º3: SAE Lensï¼ˆæœ€ã‚‚é‡è¦ï¼‰\n",
    "    print(\"\\nğŸ“‹ ãƒ•ã‚§ãƒ¼ã‚º3: SAE Lensï¼ˆé‡è¦ï¼‰\")\n",
    "    if not install_package_safely(\"sae-lens\", \"SAE Lensï¼ˆæ ¸å¿ƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰\", timeout=900):  # 15åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "        print(\"âŒ SAE Lensã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•— - ã“ã‚Œã¯è‡´å‘½çš„ã§ã™\")\n",
    "        return False\n",
    "    \n",
    "    if failed_analysis:\n",
    "        print(f\"âš ï¸ ä¸€éƒ¨ã®åˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶šè¡Œå¯èƒ½ã§ã™: {', '.join(failed_analysis)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def install_from_requirements():\n",
    "    \"\"\"requirements.txtã‹ã‚‰ã®ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰\"\"\"\n",
    "    \n",
    "    requirements_path = os.path.join(REPO_DIR, \"requirements.txt\")\n",
    "    \n",
    "    if not os.path.exists(requirements_path):\n",
    "        print(f\"âŒ requirements.txtãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {requirements_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"ğŸ“‹ requirements.txtã‹ã‚‰ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦è¡Œ...\")\n",
    "    print(\"âš ï¸ ã“ã®å‡¦ç†ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # æœ€å¤§30åˆ†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', '-q', '-r', requirements_path],\n",
    "            timeout=1800,  # 30åˆ†\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… requirements.txtä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº† ({elapsed:.1f}s)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ requirements.txtä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—\")\n",
    "            if result.stderr:\n",
    "                print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {result.stderr[:200]}...\")\n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° requirements.txtä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (30åˆ†)\")\n",
    "        print(\"ğŸ’¡ æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ requirements.txtã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¾‹å¤–: {e}\")\n",
    "        return False\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çµæœã®æ¤œè¨¼\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çµæœã‚’æ¤œè¨¼ä¸­...\")\n",
    "    \n",
    "    required_modules = {\n",
    "        'torch': 'PyTorch',\n",
    "        'transformers': 'HuggingFace Transformers',\n",
    "        'sae_lens': 'SAE Lens',\n",
    "        'plotly': 'Plotly',\n",
    "        'tqdm': 'tqdm'\n",
    "    }\n",
    "    \n",
    "    missing_modules = []\n",
    "    working_modules = []\n",
    "    \n",
    "    for module, name in required_modules.items():\n",
    "        try:\n",
    "            __import__(module)\n",
    "            working_modules.append(name)\n",
    "            print(f\"   âœ… {name}\")\n",
    "        except ImportError:\n",
    "            missing_modules.append(name)\n",
    "            print(f\"   âŒ {name}\")\n",
    "    \n",
    "    if missing_modules:\n",
    "        print(f\"\\nâš ï¸ ä¸è¶³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: {', '.join(missing_modules)}\")\n",
    "        print(f\"ğŸ’¡ ã‚»ãƒ«5ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"\\nâœ… å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ¤œè¨¼å®Œäº† ({len(working_modules)}å€‹)\")\n",
    "        return True\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å‡¦ç†\n",
    "print(\"ğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆ¦ç•¥ã®é¸æŠ\n",
    "INSTALL_STRATEGY = \"staged\"  # \"staged\" or \"bulk\"\n",
    "\n",
    "if INSTALL_STRATEGY == \"staged\":\n",
    "    print(\"ğŸ¯ æ®µéšçš„ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆ¦ç•¥ã‚’é¸æŠ\")\n",
    "    success = install_core_dependencies()\n",
    "elif INSTALL_STRATEGY == \"bulk\":\n",
    "    print(\"ğŸ¯ ä¸€æ‹¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆ¦ç•¥ã‚’é¸æŠ\") \n",
    "    success = install_from_requirements()\n",
    "else:\n",
    "    print(\"âŒ ä¸æ˜ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆ¦ç•¥\")\n",
    "    success = False\n",
    "\n",
    "# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çµæœã®æ¤œè¨¼\n",
    "if success:\n",
    "    verification_success = verify_installation()\n",
    "    \n",
    "    if verification_success:\n",
    "        print(f\"\\nğŸ‰ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†!\")\n",
    "        print(f\"ğŸ’¡ æ¬¡ã®ã‚»ãƒ« (ã‚»ãƒ«4) ã§å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚’é–‹å§‹ã—ã¦ãã ã•ã„\")\n",
    "        EXPERIMENT_SESSION['dependencies_status'] = 'ready'\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å•é¡ŒãŒã‚ã‚Šã¾ã™\")\n",
    "        print(f\"ğŸ’¡ ã‚»ãƒ«5ã®ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã§è©³ç´°ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "        EXPERIMENT_SESSION['dependencies_status'] = 'partial'\n",
    "else:\n",
    "    print(f\"\\nâŒ ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "    print(f\"ğŸ’¡ ã‚»ãƒ«5ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ©Ÿèƒ½ã‚’è©¦ã—ã¦ãã ã•ã„\")\n",
    "    EXPERIMENT_SESSION['dependencies_status'] = 'failed'\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¬ ã‚»ãƒ«4: å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰\n",
    "# ==============================================\n",
    "\n",
    "import json\n",
    "import shlex\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_jupyter_environment():\n",
    "    \"\"\"Jupyterå®Ÿè¡Œç’°å¢ƒã®è¨­å®š\"\"\"\n",
    "    print(\"ğŸ”§ Jupyterå®Ÿè¡Œç’°å¢ƒã‚’è¨­å®šä¸­...\")\n",
    "    \n",
    "    # Python ãƒ‡ãƒãƒƒã‚¬è­¦å‘Šã®æŠ‘åˆ¶\n",
    "    os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "    os.environ['PYTHONFROZEN'] = '0'\n",
    "    \n",
    "    # Jupyterè¨­å®šã®æœ€é©åŒ–\n",
    "    os.environ['JUPYTER_PLATFORM_DIRS'] = '1'\n",
    "    \n",
    "    print(\"âœ… Jupyterç’°å¢ƒè¨­å®šå®Œäº†\")\n",
    "\n",
    "def execute_experiment_notebook_direct():\n",
    "    \"\"\"å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ç›´æ¥å®Ÿè¡Œï¼ˆnbconvertã®ä»£æ›¿ï¼‰\"\"\"\n",
    "    \n",
    "    notebook_path = os.path.join(REPO_DIR, TARGET_NOTEBOOK)\n",
    "    \n",
    "    if not os.path.exists(notebook_path):\n",
    "        print(f\"âŒ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {notebook_path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"ğŸ”¬ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç›´æ¥å®Ÿè¡Œé–‹å§‹: {TARGET_NOTEBOOK}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # å®Ÿè¡Œãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "    execution_start = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Jupyter kernelç›´æ¥å®Ÿè¡Œã‚’è©¦è¡Œ\n",
    "        print(\"âš¡ Jupyter kernelç›´æ¥å®Ÿè¡Œä¸­...\")\n",
    "        print(\"â±ï¸ ã“ã®å‡¦ç†ã«ã¯æ•°åˆ†ã€œåæ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™...\")\n",
    "        \n",
    "        # å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(REPO_DIR)\n",
    "        \n",
    "        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›\n",
    "        print(f\"ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}\")\n",
    "        print(f\"ğŸ” ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯: {TARGET_NOTEBOOK}\")\n",
    "        print(f\"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª: {os.path.exists(TARGET_NOTEBOOK)}\")\n",
    "        \n",
    "        # nbconvertã‚³ãƒãƒ³ãƒ‰ã‚’ç’°å¢ƒå¤‰æ•°ä»˜ãã§å®Ÿè¡Œ\n",
    "        execute_command = [\n",
    "            'python', '-Xfrozen_modules=off', '-m', 'nbconvert',\n",
    "            '--to', 'notebook',\n",
    "            '--execute', \n",
    "            '--inplace',\n",
    "            '--allow-errors',\n",
    "            '--log-level=ERROR',  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ä¸‹ã’ã¦è­¦å‘Šã‚’æŠ‘åˆ¶\n",
    "            '--ExecutePreprocessor.timeout=2400',  # 40åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "            '--ExecutePreprocessor.kernel_name=python3',\n",
    "            TARGET_NOTEBOOK\n",
    "        ]\n",
    "        \n",
    "        print(f\"ğŸ” å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(execute_command)}\")\n",
    "        \n",
    "        # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦å®Ÿè¡Œ\n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        env['PYTHONFROZEN'] = '0'\n",
    "        \n",
    "        # å®Ÿè¡Œãƒ­ã‚°ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£\n",
    "        result = subprocess.run(\n",
    "            execute_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=2400,  # 40åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        # å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã‚‹\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        execution_end = datetime.now()\n",
    "        execution_time = (execution_end - execution_start).total_seconds()\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œå®Œäº†!\")\n",
    "            print(f\"â±ï¸ å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’ ({execution_time/60:.1f}åˆ†)\")\n",
    "            \n",
    "            # å®Ÿè¡Œçµæœã®ä¿å­˜\n",
    "            EXPERIMENT_SESSION['notebook_execution'] = {\n",
    "                'status': 'success',\n",
    "                'execution_time_seconds': execution_time,\n",
    "                'start_time': execution_start.isoformat(),\n",
    "                'end_time': execution_end.isoformat(),\n",
    "                'method': 'direct_nbconvert'\n",
    "            }\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼\")\n",
    "            print(f\"ğŸ’¡ ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰: {result.returncode}\")\n",
    "            \n",
    "            # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã®è©³ç´°è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚¬è­¦å‘Šã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰\n",
    "            if result.stderr:\n",
    "                error_lines = result.stderr.split('\\n')\n",
    "                filtered_errors = [line for line in error_lines \n",
    "                                 if not ('Debugger warning' in line or 'frozen modules' in line)]\n",
    "                if filtered_errors:\n",
    "                    print(f\"ğŸ” é‡è¦ãªã‚¨ãƒ©ãƒ¼è©³ç´°:\")\n",
    "                    for line in filtered_errors[:10]:  # æœ€åˆã®10è¡Œã®ã¿\n",
    "                        if line.strip():\n",
    "                            print(f\"   {line}\")\n",
    "            \n",
    "            if result.stdout:\n",
    "                print(f\"ğŸ“‹ å®Ÿè¡Œå‡ºåŠ›ï¼ˆæŠœç²‹ï¼‰:\")\n",
    "                stdout_lines = result.stdout.split('\\n')\n",
    "                for line in stdout_lines[-5:]:  # æœ€å¾Œã®5è¡Œã®ã¿\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "            \n",
    "            EXPERIMENT_SESSION['notebook_execution'] = {\n",
    "                'status': 'failed',\n",
    "                'error': result.stderr if result.stderr else 'Unknown error',\n",
    "                'stdout': result.stdout if result.stdout else '',\n",
    "                'returncode': result.returncode,\n",
    "                'execution_time_seconds': execution_time,\n",
    "                'method': 'direct_nbconvert'\n",
    "            }\n",
    "            \n",
    "            return False\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œ - ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (40åˆ†)\")\n",
    "        os.chdir(original_dir)  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…ƒã«æˆ»ã™\n",
    "        EXPERIMENT_SESSION['notebook_execution'] = {\n",
    "            'status': 'timeout',\n",
    "            'timeout_seconds': 2400,\n",
    "            'method': 'direct_nbconvert'\n",
    "        }\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œä¾‹å¤–: {e}\")\n",
    "        os.chdir(original_dir)  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…ƒã«æˆ»ã™\n",
    "        EXPERIMENT_SESSION['notebook_execution'] = {\n",
    "            'status': 'exception',\n",
    "            'error': str(e),\n",
    "            'method': 'direct_nbconvert'\n",
    "        }\n",
    "        return False\n",
    "\n",
    "def execute_experiment_notebook_alternative():\n",
    "    \"\"\"ä»£æ›¿æ‰‹æ®µã«ã‚ˆã‚‹å®Ÿé¨“å®Ÿè¡Œï¼ˆPythonç›´æ¥å®Ÿè¡Œï¼‰\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ ä»£æ›¿å®Ÿè¡Œæ–¹æ³•ã‚’è©¦è¡Œä¸­...\")\n",
    "    \n",
    "    # Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã—ã¦ç›´æ¥å®Ÿè¡Œã‚’è©¦è¡Œ\n",
    "    analyzer_path = os.path.join(REPO_DIR, \"sycophancy_analyzer.py\")\n",
    "    \n",
    "    if not os.path.exists(analyzer_path):\n",
    "        print(f\"âŒ åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {analyzer_path}\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(REPO_DIR)\n",
    "        \n",
    "        print(\"ğŸ Pythonç›´æ¥å®Ÿè¡Œã‚’è©¦è¡Œä¸­...\")\n",
    "        \n",
    "        # Pythonç’°å¢ƒã§ç›´æ¥å®Ÿè¡Œ\n",
    "        execute_command = [\n",
    "            'python', '-Xfrozen_modules=off', 'sycophancy_analyzer.py'\n",
    "        ]\n",
    "        \n",
    "        env = os.environ.copy()\n",
    "        env['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            execute_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=1800,  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ\n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Pythonç›´æ¥å®Ÿè¡ŒæˆåŠŸ!\")\n",
    "            EXPERIMENT_SESSION['notebook_execution'] = {\n",
    "                'status': 'success',\n",
    "                'method': 'python_direct'\n",
    "            }\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Pythonç›´æ¥å®Ÿè¡Œå¤±æ•—: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä»£æ›¿å®Ÿè¡Œä¾‹å¤–: {e}\")\n",
    "        os.chdir(original_dir)\n",
    "        return False\n",
    "\n",
    "def check_experiment_results():\n",
    "    \"\"\"å®Ÿé¨“çµæœã®ç¢ºèª\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” å®Ÿé¨“çµæœã‚’ç¢ºèªä¸­...\")\n",
    "    \n",
    "    results_dir = os.path.join(REPO_DIR, \"results\")\n",
    "    plots_dir = os.path.join(REPO_DIR, \"plots\")\n",
    "    \n",
    "    found_results = []\n",
    "    \n",
    "    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    if os.path.exists(results_dir):\n",
    "        result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
    "        if result_files:\n",
    "            print(f\"ğŸ“Š çµæœãƒ•ã‚¡ã‚¤ãƒ« ({len(result_files)}å€‹):\")\n",
    "            for file in result_files[:3]:  # æœ€åˆã®3å€‹ã®ã¿è¡¨ç¤º\n",
    "                file_path = os.path.join(results_dir, file)\n",
    "                size_kb = os.path.getsize(file_path) / 1024\n",
    "                print(f\"   ğŸ“„ {file} ({size_kb:.1f}KB)\")\n",
    "                found_results.append(file)\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
    "    if os.path.exists(plots_dir):\n",
    "        plot_files = [f for f in os.listdir(plots_dir) if f.endswith('.html')]\n",
    "        if plot_files:\n",
    "            print(f\"ğŸ“ˆ å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ« ({len(plot_files)}å€‹):\")\n",
    "            for file in plot_files[:3]:  # æœ€åˆã®3å€‹ã®ã¿è¡¨ç¤º\n",
    "                print(f\"   ğŸ“Š {file}\")\n",
    "                found_results.append(file)\n",
    "    \n",
    "    return len(found_results) > 0\n",
    "\n",
    "def save_experiment_log():\n",
    "    \"\"\"å®Ÿé¨“ãƒ­ã‚°ã®ä¿å­˜\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ å®Ÿé¨“ãƒ­ã‚°ã‚’ä¿å­˜ä¸­...\")\n",
    "    \n",
    "    # å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†æ™‚åˆ»ã®è¨˜éŒ²\n",
    "    EXPERIMENT_SESSION['end_time'] = datetime.now().isoformat()\n",
    "    EXPERIMENT_SESSION['total_duration_seconds'] = (\n",
    "        datetime.fromisoformat(EXPERIMENT_SESSION['end_time']) - \n",
    "        datetime.fromisoformat(EXPERIMENT_SESSION['start_time'])\n",
    "    ).total_seconds()\n",
    "    \n",
    "    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "    log_filename = f\"experiment_log_{EXPERIMENT_SESSION['session_id']}.json\"\n",
    "    log_path = os.path.join(REPO_DIR, log_filename)\n",
    "    \n",
    "    try:\n",
    "        with open(log_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(EXPERIMENT_SESSION, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"âœ… å®Ÿé¨“ãƒ­ã‚°ä¿å­˜: {log_filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return False\n",
    "\n",
    "def display_experiment_summary():\n",
    "    \"\"\"å®Ÿé¨“ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ‰ å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"ğŸ“‹ å®Ÿé¨“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:\")\n",
    "    print(f\"   ğŸ”– ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {EXPERIMENT_SESSION['session_id']}\")\n",
    "    print(f\"   ğŸŒ¿ ãƒ–ãƒ©ãƒ³ãƒ: {EXPERIMENT_SESSION['branch']}\")\n",
    "    print(f\"   ğŸ“Œ ã‚³ãƒŸãƒƒãƒˆ: {EXPERIMENT_SESSION.get('commit_hash', 'unknown')}\")\n",
    "    print(f\"   ğŸ• é–‹å§‹æ™‚åˆ»: {EXPERIMENT_SESSION['start_time']}\")\n",
    "    print(f\"   ğŸ• çµ‚äº†æ™‚åˆ»: {EXPERIMENT_SESSION.get('end_time', 'unknown')}\")\n",
    "    \n",
    "    if 'total_duration_seconds' in EXPERIMENT_SESSION:\n",
    "        duration = EXPERIMENT_SESSION['total_duration_seconds']\n",
    "        print(f\"   â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†)\")\n",
    "    \n",
    "    # å„æ®µéšã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n",
    "    print(f\"\\nğŸ“Š å„æ®µéšã®å®Ÿè¡ŒçŠ¶æ³:\")\n",
    "    print(f\"   ğŸ”§ ç’°å¢ƒæ§‹ç¯‰: {EXPERIMENT_SESSION.get('environment_status', 'unknown')}\")\n",
    "    print(f\"   ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒª: {EXPERIMENT_SESSION.get('repository_status', 'unknown')}\")\n",
    "    print(f\"   ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {EXPERIMENT_SESSION.get('dataset_status', 'unknown')}\")\n",
    "    print(f\"   ğŸ“¦ ä¾å­˜é–¢ä¿‚: {EXPERIMENT_SESSION.get('dependencies_status', 'unknown')}\")\n",
    "    \n",
    "    if 'notebook_execution' in EXPERIMENT_SESSION:\n",
    "        nb_status = EXPERIMENT_SESSION['notebook_execution']['status']\n",
    "        nb_method = EXPERIMENT_SESSION['notebook_execution'].get('method', 'unknown')\n",
    "        print(f\"   ğŸ”¬ å®Ÿé¨“å®Ÿè¡Œ: {nb_status} ({nb_method})\")\n",
    "        \n",
    "        if nb_status == 'success' and 'execution_time_seconds' in EXPERIMENT_SESSION['notebook_execution']:\n",
    "            nb_time = EXPERIMENT_SESSION['notebook_execution']['execution_time_seconds']\n",
    "            print(f\"      â±ï¸ å®Ÿé¨“æ™‚é–“: {nb_time:.1f}ç§’ ({nb_time/60:.1f}åˆ†)\")\n",
    "    \n",
    "    # GPUä½¿ç”¨çŠ¶æ³\n",
    "    if EXPERIMENT_SESSION.get('gpu_available'):\n",
    "        print(f\"   ğŸš€ GPU: {EXPERIMENT_SESSION.get('gpu_name', 'unknown')} \"\n",
    "              f\"({EXPERIMENT_SESSION.get('gpu_memory_gb', 'unknown')}GB)\")\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³å®Ÿé¨“å®Ÿè¡Œå‡¦ç†\n",
    "print(\"ğŸ”¬ å®Ÿé¨“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ã‚’é–‹å§‹...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å‰ææ¡ä»¶ã®ç¢ºèª\n",
    "if EXPERIMENT_SESSION.get('dependencies_status') not in ['ready', 'partial']:\n",
    "    print(\"âŒ ä¾å­˜é–¢ä¿‚ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"ğŸ’¡ ã‚»ãƒ«3ã§ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’å®Œäº†ã—ã¦ãã ã•ã„\")\n",
    "elif EXPERIMENT_SESSION.get('dataset_status') not in ['ready', 'ready_copied']:\n",
    "    print(\"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "    print(\"ğŸ’¡ ã‚»ãƒ«2.5ã§ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã‚’å®Œäº†ã—ã¦ãã ã•ã„\")\n",
    "else:\n",
    "    # Jupyterç’°å¢ƒè¨­å®š\n",
    "    setup_jupyter_environment()\n",
    "    \n",
    "    # å®Ÿé¨“å®Ÿè¡Œï¼ˆãƒ¡ã‚¤ãƒ³æ‰‹æ³•ï¼‰\n",
    "    print(\"\\nğŸ¯ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œæ‰‹æ³•: æ”¹è‰¯ã•ã‚ŒãŸnbconvert\")\n",
    "    execution_success = execute_experiment_notebook_direct()\n",
    "    \n",
    "    # ãƒ¡ã‚¤ãƒ³æ‰‹æ³•ãŒå¤±æ•—ã—ãŸå ´åˆã®ä»£æ›¿å®Ÿè¡Œ\n",
    "    if not execution_success:\n",
    "        print(\"\\nğŸ”„ ä»£æ›¿å®Ÿè¡Œæ‰‹æ³•ã‚’è©¦è¡Œ...\")\n",
    "        execution_success = execute_experiment_notebook_alternative()\n",
    "    \n",
    "    if execution_success:\n",
    "        # çµæœç¢ºèª\n",
    "        results_found = check_experiment_results()\n",
    "        \n",
    "        if results_found:\n",
    "            print(\"âœ… å®Ÿé¨“çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã—ãŸ\")\n",
    "        else:\n",
    "            print(\"âš ï¸ å®Ÿé¨“çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
    "            print(\"ğŸ’¡ å®Ÿé¨“ã¯å®Œäº†ã—ã¾ã—ãŸãŒã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™\")\n",
    "        \n",
    "        # ãƒ­ã‚°ä¿å­˜\n",
    "        save_experiment_log()\n",
    "        \n",
    "        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º\n",
    "        display_experiment_summary()\n",
    "        \n",
    "        print(f\"\\nğŸ‰ å®Ÿé¨“å®Œäº†!\")\n",
    "        print(f\"ğŸ’¡ çµæœã¯ {REPO_DIR}/results/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„\")\n",
    "        print(f\"ğŸ“Š å¯è¦–åŒ–çµæœã¯ {REPO_DIR}/plots/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚Šã¾ã™\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ å…¨ã¦ã®å®Ÿé¨“å®Ÿè¡Œæ‰‹æ³•ãŒå¤±æ•—ã—ã¾ã—ãŸ\")\n",
    "        print(\"ğŸ’¡ ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\")\n",
    "        print(\"   1. ã‚»ãƒ«5ã®ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ã§è©³ç´°ç¢ºèª\")\n",
    "        print(\"   2. ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚»ãƒ«3ï¼‰\")\n",
    "        print(\"   3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç¢ºèªï¼ˆã‚»ãƒ«2.5ï¼‰\")\n",
    "        \n",
    "        # ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¡¨ç¤º\n",
    "        if 'notebook_execution' in EXPERIMENT_SESSION:\n",
    "            nb_exec = EXPERIMENT_SESSION['notebook_execution']\n",
    "            if 'error' in nb_exec:\n",
    "                print(f\"\\nğŸ” æœ€æ–°ã‚¨ãƒ©ãƒ¼æƒ…å ±:\")\n",
    "                error_text = nb_exec['error']\n",
    "                # é‡è¦ãªã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚¬è­¦å‘Šã‚’é™¤ãï¼‰\n",
    "                error_lines = error_text.split('\\n') if error_text else []\n",
    "                important_errors = [line for line in error_lines[:10] \n",
    "                                  if line.strip() and not ('Debugger warning' in line or 'frozen modules' in line)]\n",
    "                for line in important_errors:\n",
    "                    print(f\"   {line}\")\n",
    "                    \n",
    "        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ï¼ˆæ¬¡å›å®Ÿè¡Œã®ãŸã‚ï¼‰\n",
    "        print(f\"\\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ã‚’å®Ÿè¡Œä¸­...\")\n",
    "        try:\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            if 'torch' in sys.modules:\n",
    "                import torch\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            print(\"âœ… ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Œäº†\")\n",
    "        except:\n",
    "            print(\"âš ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ã«å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸ\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ã‚»ãƒ«5: ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n",
    "# ==============================================\n",
    "\n",
    "def debug_environment():\n",
    "    \"\"\"ç’°å¢ƒãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®è¡¨ç¤º\"\"\"\n",
    "    \n",
    "    print(\"ğŸ” ç’°å¢ƒãƒ‡ãƒãƒƒã‚°æƒ…å ±\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Pythonç’°å¢ƒ\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path[:3]}...\")  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º\n",
    "    \n",
    "    # Google Colabç’°å¢ƒ\n",
    "    try:\n",
    "        import google.colab\n",
    "        print(\"Google Colab: âœ… åˆ©ç”¨å¯èƒ½\")\n",
    "        \n",
    "        # GPUæƒ…å ±\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                print(f\"GPU: âœ… {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")\n",
    "                \n",
    "                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³\n",
    "                allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "                reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "                print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "            else:\n",
    "                print(\"GPU: âŒ åˆ©ç”¨ä¸å¯\")\n",
    "        except ImportError:\n",
    "            print(\"PyTorch: âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"Google Colab: âŒ ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ\")\n",
    "    \n",
    "    # ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡\n",
    "    import shutil\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    print(f\"Disk - Total: {total//(1024**3)}GB, Free: {free//(1024**3)}GB\")\n",
    "\n",
    "def debug_repository_status():\n",
    "    \"\"\"ãƒªãƒã‚¸ãƒˆãƒªçŠ¶æ…‹ã®ãƒ‡ãƒãƒƒã‚°\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‚ ãƒªãƒã‚¸ãƒˆãƒªãƒ‡ãƒãƒƒã‚°æƒ…å ±\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if os.path.exists(REPO_DIR):\n",
    "        print(f\"âœ… ãƒªãƒã‚¸ãƒˆãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨: {REPO_DIR}\")\n",
    "        \n",
    "        # Gitæƒ…å ±\n",
    "        original_dir = os.getcwd()\n",
    "        try:\n",
    "            os.chdir(REPO_DIR)\n",
    "            \n",
    "            # ãƒ–ãƒ©ãƒ³ãƒç¢ºèª\n",
    "            branch_info = !git branch --show-current\n",
    "            print(f\"Current branch: {branch_info[0] if branch_info else 'unknown'}\")\n",
    "            \n",
    "            # ã‚³ãƒŸãƒƒãƒˆæƒ…å ±\n",
    "            commit_info = !git log --oneline -1\n",
    "            print(f\"Latest commit: {commit_info[0] if commit_info else 'unknown'}\")\n",
    "            \n",
    "            # sparse-checkoutçŠ¶æ…‹\n",
    "            sparse_info = !git sparse-checkout list\n",
    "            if sparse_info:\n",
    "                print(f\"Sparse-checkout files: {len(sparse_info)} items\")\n",
    "                for item in sparse_info[:5]:  # æœ€åˆã®5ã¤ã®ã¿è¡¨ç¤º\n",
    "                    print(f\"  - {item}\")\n",
    "            \n",
    "            os.chdir(original_dir)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Gitæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            os.chdir(original_dir)\n",
    "        \n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª\n",
    "        print(\"\\nãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:\")\n",
    "        required_files = [\"sycophancy_analyzer.py\", \"config.py\", TARGET_NOTEBOOK]\n",
    "        for file in required_files:\n",
    "            file_path = os.path.join(REPO_DIR, file)\n",
    "            if os.path.exists(file_path):\n",
    "                size = os.path.getsize(file_path) / 1024\n",
    "                print(f\"  âœ… {file} ({size:.1f}KB)\")\n",
    "            else:\n",
    "                print(f\"  âŒ {file} (ä¸å­˜åœ¨)\")\n",
    "    else:\n",
    "        print(f\"âŒ ãƒªãƒã‚¸ãƒˆãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {REPO_DIR}\")\n",
    "\n",
    "def debug_dependencies():\n",
    "    \"\"\"ä¾å­˜é–¢ä¿‚ã®ãƒ‡ãƒãƒƒã‚°\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“¦ ä¾å­˜é–¢ä¿‚ãƒ‡ãƒãƒƒã‚°æƒ…å ±\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    critical_modules = {\n",
    "        'torch': 'PyTorch',\n",
    "        'transformers': 'HuggingFace Transformers', \n",
    "        'sae_lens': 'SAE Lens',\n",
    "        'accelerate': 'Accelerate',\n",
    "        'plotly': 'Plotly',\n",
    "        'tqdm': 'tqdm',\n",
    "        'numpy': 'NumPy',\n",
    "        'pandas': 'Pandas'\n",
    "    }\n",
    "    \n",
    "    print(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç¢ºèª:\")\n",
    "    for module, name in critical_modules.items():\n",
    "        try:\n",
    "            imported_module = __import__(module)\n",
    "            version = getattr(imported_module, '__version__', 'unknown')\n",
    "            print(f\"  âœ… {name}: v{version}\")\n",
    "        except ImportError:\n",
    "            print(f\"  âŒ {name}: ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ {name}: ã‚¨ãƒ©ãƒ¼ ({e})\")\n",
    "\n",
    "def manual_install_dependencies():\n",
    "    \"\"\"æ‰‹å‹•ã§ã®ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”§ æ‰‹å‹•ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    critical_packages = [\n",
    "        \"torch\",\n",
    "        \"transformers\", \n",
    "        \"sae-lens\",\n",
    "        \"accelerate\",\n",
    "        \"plotly\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "    \n",
    "    print(\"é‡è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "    \n",
    "    for package in critical_packages:\n",
    "        print(f\"\\nğŸ“¦ {package} ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\")\n",
    "        try:\n",
    "            result = !pip install -q {package}\n",
    "            print(f\"  âœ… {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çµæœã‚’ç¢ºèª...\")\n",
    "    debug_dependencies()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢å®Ÿè¡Œä¸­...\")\n",
    "    \n",
    "    # Python garbage collection\n",
    "    import gc\n",
    "    collected = gc.collect()\n",
    "    print(f\"  ğŸ—‘ï¸ Python GC: {collected} ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå›å\")\n",
    "    \n",
    "    # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            \n",
    "            allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "            reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "            print(f\"  ğŸš€ GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸ PyTorchåˆ©ç”¨ä¸å¯ - GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢ã‚¹ã‚­ãƒƒãƒ—\")\n",
    "\n",
    "def display_experiment_session():\n",
    "    \"\"\"å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã®è¡¨ç¤º\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'EXPERIMENT_SESSION' in globals():\n",
    "        for key, value in EXPERIMENT_SESSION.items():\n",
    "            if isinstance(value, dict):\n",
    "                print(f\"{key}:\")\n",
    "                for sub_key, sub_value in value.items():\n",
    "                    print(f\"  {sub_key}: {sub_value}\")\n",
    "            else:\n",
    "                print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“\")\n",
    "\n",
    "def run_full_diagnostic():\n",
    "    \"\"\"å®Œå…¨è¨ºæ–­ã®å®Ÿè¡Œ\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¬ å®Œå…¨è¨ºæ–­ã‚’å®Ÿè¡Œä¸­...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    debug_environment()\n",
    "    print()\n",
    "    debug_repository_status() \n",
    "    print()\n",
    "    debug_dependencies()\n",
    "    print()\n",
    "    display_experiment_session()\n",
    "    print()\n",
    "    clear_memory()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… å®Œå…¨è¨ºæ–­å®Œäº†\")\n",
    "\n",
    "# ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ãƒ¡ãƒ‹ãƒ¥ãƒ¼\n",
    "print(\"ğŸ”§ ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½:\")\n",
    "print(\"1. run_full_diagnostic() - å®Œå…¨ã‚·ã‚¹ãƒ†ãƒ è¨ºæ–­\")\n",
    "print(\"2. debug_environment() - ç’°å¢ƒæƒ…å ±ç¢ºèª\")\n",
    "print(\"3. debug_repository_status() - ãƒªãƒã‚¸ãƒˆãƒªçŠ¶æ…‹ç¢ºèª\")\n",
    "print(\"4. debug_dependencies() - ä¾å­˜é–¢ä¿‚ç¢ºèª\")\n",
    "print(\"5. manual_install_dependencies() - æ‰‹å‹•ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\")\n",
    "print(\"6. clear_memory() - ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\")\n",
    "print(\"7. display_experiment_session() - å®Ÿé¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±è¡¨ç¤º\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:\")\n",
    "print(f\"   å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã¾ãš run_full_diagnostic() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\")\n",
    "print(f\"   ç‰¹å®šã®å•é¡Œã«å¯¾ã—ã¦ã¯å€‹åˆ¥ã®è¨ºæ–­æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„\")\n",
    "\n",
    "print(f\"\\nğŸš¨ ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºç­–:\")\n",
    "print(f\"   ğŸ“¦ ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼ â†’ manual_install_dependencies()\")\n",
    "print(f\"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä¸è¶³ â†’ clear_memory()\")\n",
    "print(f\"   ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¶³ â†’ debug_repository_status()\")\n",
    "print(f\"   ğŸ ç’°å¢ƒå•é¡Œ â†’ debug_environment()\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# è‡ªå‹•çš„ã«åŸºæœ¬è¨ºæ–­ã‚’å®Ÿè¡Œ\n",
    "print(\"ğŸ” åŸºæœ¬è¨ºæ–­ã‚’è‡ªå‹•å®Ÿè¡Œä¸­...\")\n",
    "debug_environment()\n",
    "print()\n",
    "if os.path.exists(REPO_DIR):\n",
    "    debug_repository_status()\n",
    "print(\"âœ… åŸºæœ¬è¨ºæ–­å®Œäº†\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
